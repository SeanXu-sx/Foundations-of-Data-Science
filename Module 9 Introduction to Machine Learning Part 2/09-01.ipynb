{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: Introduction to Machine Learning Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Learning Outcomes</a></span></li><li><span><a href=\"#Readings-and-Resources\" data-toc-modified-id=\"Readings-and-Resources-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Readings and Resources</a></span></li><li><span><a href=\"#The-Space-Shuttle-Challenger-Explosion-and-the-O-Rings\" data-toc-modified-id=\"The-Space-Shuttle-Challenger-Explosion-and-the-O-Rings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>The Space Shuttle Challenger Explosion and the O-Rings</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"></ul></li><li><span><a href=\"#Dummy-Variables\" data-toc-modified-id=\"Dummy-Variables-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dummy Variables</a></span></li><li><span><a href=\"#Training-vs-Test-datasets\" data-toc-modified-id=\"Training-vs-Test-datasets-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training vs Test datasets</a></span></li><li><span><a href=\"#Introduction-to-Other-Modeling-Techniques\" data-toc-modified-id=\"Introduction-to-Other-Modeling-Techniques-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Introduction to Other Modeling Techniques</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module you will continue to learn and practice:\n",
    "* Linear Regression\n",
    "* Feature Engineering\n",
    "* Dummy Variables\n",
    "* Testing / Training Models\n",
    "* Other Modeling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the notebook content draws from the recommended readings. We invite you to further supplement this notebook with the following recommended texts.\n",
    "\n",
    "Geron, A. (2017) *Hands-on Machine Learning with Scikit-Learn and TensorFlow*. O'Reilly Media.\n",
    "\n",
    "Witten, I.H, Frank, E. (2005) *Data Mining. Practical Machine Learning Tools and Techniques* (2nd edition). Elsevier.\n",
    "\n",
    "`statsmodels` Documentation can be found at https://www.statsmodels.org/dev/index.html.\n",
    "\n",
    "`scikit-learn` Documentation can be found at http://scikit-learn.org/stable/documentation.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module continues the introduction to Machine Learning. It will cover the qualitative aspects of Machine Learning, including feature engineering, dummy variables, model validation, and introduction to other modeling techniques.\n",
    "\n",
    "We will begin by refreshing our knowledge of linear regressions by applying the concepts to a new problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Space Shuttle Challenger Explosion and the O-Rings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a data set about the Challenger space shuttle disaster of 1986, when one of the rocket boosters exploded. After the data was analyzed, the commission determined that the explosion was caused by the failure of an O-ring in the rocket booster which was unacceptably sensitive to the outside temperature and other factors. The dataset contains data from 24 flights prior to the disaster.\n",
    "\n",
    "The dataset home page can be found [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Challenger+USA+Space+Shuttle+O-Ring).\n",
    "You can also read about the event in [Wikipedia](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster).\n",
    "\n",
    "First, we will import the relevant modeling packages and the data set. The definitions for each column are as follows:\n",
    "\n",
    "* ORings: Number of O-rings at risk on a given flight\n",
    "* DistressedOrings: Number experiencing thermal distress\n",
    "* Temp: Launch temperature (degrees F)\n",
    "* Pressure: Leak-check pressure (psi)\n",
    "* TempOrderOfFlight: Temporal order of flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORings</th>\n",
       "      <th>DistressedOrings</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>TempOrderOfFlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>200</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>200</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>200</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>200</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>200</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>200</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>200</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ORings  DistressedOrings  Temp  Pressure  TempOrderOfFlight\n",
       "0        6                 0    66        50                  1\n",
       "1        6                 1    70        50                  2\n",
       "2        6                 0    69        50                  3\n",
       "3        6                 0    68        50                  4\n",
       "4        6                 0    67        50                  5\n",
       "5        6                 0    72        50                  6\n",
       "6        6                 0    73       100                  7\n",
       "7        6                 0    70       100                  8\n",
       "8        6                 1    57       200                  9\n",
       "9        6                 1    63       200                 10\n",
       "10       6                 1    70       200                 11\n",
       "11       6                 0    78       200                 12\n",
       "12       6                 0    67       200                 13\n",
       "13       6                 2    53       200                 14\n",
       "14       6                 0    67       200                 15\n",
       "15       6                 0    75       200                 16\n",
       "16       6                 0    70       200                 17\n",
       "17       6                 0    81       200                 18\n",
       "18       6                 0    76       200                 19\n",
       "19       6                 0    79       200                 20\n",
       "20       6                 2    75       200                 21\n",
       "21       6                 0    76       200                 22\n",
       "22       6                 1    58       200                 23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# import formula api as alias smf\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "cols = ['ORings', 'DistressedOrings', 'Temp', 'Pressure', 'TempOrderOfFlight']\n",
    "\n",
    "df = pd.read_csv('o-ring-erosion-or-blowby.csv', names=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will run an Ordinary Least Squares (OLS) Regression, which is one of the simplest methods of linear regression. But what does it mean?\n",
    "\n",
    "Well, we know there is rarely a perfect relationship when we are modeling one or more independent variables and their impact on a dependent variable. In this example, there is no *exact* temperature that will *always* result in a distressed o-ring. Sometimes there will be errors in output - this is referred to as the error rate. And because there is no *perfect* relationship, there are many lines of best fit which can represent a relationship between two or more variables.\n",
    "\n",
    "The objective of an OLS regression is to produce the one straight line with the least squared errors, to provide us with the most accurate prediction possible by using the linear regression method.\n",
    "\n",
    "Let's review the variables which make up our model:\n",
    "\n",
    "* **O-Rings:** This is a constant (its always 6) so there is no point using it as a predictor. It doesn't vary so it can't contribute to different cases having different outcomes.\n",
    "\n",
    "* **DistressedOrings:** This is what we're trying to predict so this is our target variable\n",
    " \n",
    "* **Temp:** Our most important predictor\n",
    "\n",
    "* **Pressure:** Might or might not be predictive. Include it and see what happens\n",
    "\n",
    "* **TempOrderOfFlight:** This is just the order of the flights (Flight #1, #2, etc.). If we were interested in whether the situation is getting better or worse over time we want to include this as a predictor but since we are only interested in the effects of temperature (and possibly test pressure) including this might result in the model attributing the change in # of rings to just the passage of time and mask the relationship we're really interested in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/irina/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>DistressedOrings</td> <th>  R-squared:         </th> <td>   0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 16 Jan 2020</td> <th>  Prob (F-statistic):</th>  <td>0.0126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:44:21</td>     <th>  Log-Likelihood:    </th> <td> -17.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    23</td>      <th>  AIC:               </th> <td>   40.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   44.22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    3.3298</td> <td>    1.188</td> <td>    2.803</td> <td> 0.011</td> <td>    0.851</td> <td>    5.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Temp</th>     <td>   -0.0487</td> <td>    0.017</td> <td>   -2.910</td> <td> 0.009</td> <td>   -0.084</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pressure</th> <td>    0.0029</td> <td>    0.002</td> <td>    1.699</td> <td> 0.105</td> <td>   -0.001</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>19.324</td> <th>  Durbin-Watson:     </th> <td>   2.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  23.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.782</td> <th>  Prob(JB):          </th> <td>8.00e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.433</td> <th>  Cond. No.          </th> <td>1.84e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.84e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:       DistressedOrings   R-squared:                       0.354\n",
       "Model:                            OLS   Adj. R-squared:                  0.290\n",
       "Method:                 Least Squares   F-statistic:                     5.490\n",
       "Date:                Thu, 16 Jan 2020   Prob (F-statistic):             0.0126\n",
       "Time:                        01:44:21   Log-Likelihood:                -17.408\n",
       "No. Observations:                  23   AIC:                             40.82\n",
       "Df Residuals:                      20   BIC:                             44.22\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.3298      1.188      2.803      0.011       0.851       5.808\n",
       "Temp          -0.0487      0.017     -2.910      0.009      -0.084      -0.014\n",
       "Pressure       0.0029      0.002      1.699      0.105      -0.001       0.007\n",
       "==============================================================================\n",
       "Omnibus:                       19.324   Durbin-Watson:                   2.390\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.471\n",
       "Skew:                           1.782   Prob(JB):                     8.00e-06\n",
       "Kurtosis:                       6.433   Cond. No.                     1.84e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.84e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['Temp', 'Pressure']]\n",
    "y = df['DistressedOrings']\n",
    "\n",
    "# Add a constant so the model will choose an intercept. (Otherwise the model will fit a line through the origin).\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS model\n",
    "est = sm.OLS(y, X).fit()\n",
    "\n",
    "# Check the results\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our regression results, we can use the `params` function to find the values of our slopes and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const       3.329831\n",
       "Temp       -0.048671\n",
       "Pressure    0.002939\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these values and the line which best represents the relationship to predict outcomes based on different levels of pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp=31 Pressure= 0  Predicted # of O-Rings in distress: 1.8210269508611583\n",
      "Temp=31 Pressure= 50  Predicted # of O-Rings in distress: 1.9679931836796445\n",
      "Temp=31 Pressure= 100  Predicted # of O-Rings in distress: 2.114959416498131\n",
      "Temp=31 Pressure= 200  Predicted # of O-Rings in distress: 2.4088918821351033\n"
     ]
    }
   ],
   "source": [
    "# Intercept\n",
    "constant = est.params[0] \n",
    "# Coeff for Temp\n",
    "coef1 = est.params[1]\n",
    "# Coeff for Pressure\n",
    "coef2 = est.params[2]\n",
    "\n",
    "# No. of O rings in distress when temperature = 31 and pressure is 0, 50, 100, and 200\n",
    "for pressure in [0, 50, 100, 200]:\n",
    "    print(\"Temp=31 Pressure=\", pressure, \" Predicted # of O-Rings in distress:\", constant + coef1 * 31 + coef2 * pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.82102695, 1.96799318, 2.11495942, 2.40889188])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or using predict()\n",
    "est.predict([[1, 31, 0], [1, 31, 50], [1, 31, 100], [1, 31, 200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "If we assume the overall relationship is linear the analysis suggests that approximately 2 o-rings will experience distress if the temperature the day of the launch is 31F.  See notes below for important cautions.\n",
    "\n",
    "### Notes\n",
    "\n",
    "Extrapolating results outside the range of actual observations like we are doing here is always a very dicey proposition. It assumes that the overall relationship is truly linear but any smooth curve looked at over a small enough range will look straight--this is why calculus works.\n",
    "\n",
    "In the absence of any better way to do this (there were only so many actual flights) it can provide some insight but must be thought of as indicative only, not an actual prediction with any precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models using R-style formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodel.api` function allows us to fit our regression model in the form of a matrix. Using `statsmodel.formula.api` will allow us to fit the same model using formula arguments. This is comparable to other programming languages, including R.\n",
    "\n",
    "You can find more information about this library here: http://www.statsmodels.org/dev/example_formulas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# formula: response ~ predictors\n",
    "est = smf.ols(formula='DistressedOrings ~ Temp+Pressure', data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>DistressedOrings</td> <th>  R-squared:         </th> <td>   0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 16 Jan 2020</td> <th>  Prob (F-statistic):</th>  <td>0.0126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:44:38</td>     <th>  Log-Likelihood:    </th> <td> -17.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    23</td>      <th>  AIC:               </th> <td>   40.82</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>   44.22</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    3.3298</td> <td>    1.188</td> <td>    2.803</td> <td> 0.011</td> <td>    0.851</td> <td>    5.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Temp</th>      <td>   -0.0487</td> <td>    0.017</td> <td>   -2.910</td> <td> 0.009</td> <td>   -0.084</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Pressure</th>  <td>    0.0029</td> <td>    0.002</td> <td>    1.699</td> <td> 0.105</td> <td>   -0.001</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>19.324</td> <th>  Durbin-Watson:     </th> <td>   2.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  23.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.782</td> <th>  Prob(JB):          </th> <td>8.00e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 6.433</td> <th>  Cond. No.          </th> <td>1.84e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.84e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:       DistressedOrings   R-squared:                       0.354\n",
       "Model:                            OLS   Adj. R-squared:                  0.290\n",
       "Method:                 Least Squares   F-statistic:                     5.490\n",
       "Date:                Thu, 16 Jan 2020   Prob (F-statistic):             0.0126\n",
       "Time:                        01:44:38   Log-Likelihood:                -17.408\n",
       "No. Observations:                  23   AIC:                             40.82\n",
       "Df Residuals:                      20   BIC:                             44.22\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      3.3298      1.188      2.803      0.011       0.851       5.808\n",
       "Temp          -0.0487      0.017     -2.910      0.009      -0.084      -0.014\n",
       "Pressure       0.0029      0.002      1.699      0.105      -0.001       0.007\n",
       "==============================================================================\n",
       "Omnibus:                       19.324   Durbin-Watson:                   2.390\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.471\n",
       "Skew:                           1.782   Prob(JB):                     8.00e-06\n",
       "Kurtosis:                       6.433   Cond. No.                     1.84e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.84e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the calculations are exactly the same as in Part 1 of the solution above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models use input data to produce a certain output. Feature engineering refers to the act of creating an input dataset which is relevant and compatible with the learning algorithm requirements. For example, let's assume you're building a model to predict house prices based on the following data points: number of rooms, area, latitude, and longitude. Perhaps you can use number of rooms or area as individual variables, however latitude and longitude don't have much meaning on their own. This is where feature engineering comes in - you can combine the latitude and longitute values to create defined location coordinates. This information is significantly more useful than either data point on its own.\n",
    "\n",
    "There are a number of techniques we can use to engineer features. We will review them briefly in this module and you will learn to apply them as you progress through the certificate.\n",
    "\n",
    "* **Imputation:** Missing values are a common problem as you prepare your data for machine learning. The simplest solution to dealing with missing values is to drop rows or columns that have a significant number of missing values. Numerical Imputation is another method of handling missing values, where you assign a numerical value through assumption, or by using the mean,  median or most of a column.\n",
    "\n",
    "* **Detecting Outliers:** Outliers, or individual data points that lie outside of the data distribution, can skew your results. This is why it is important to remove any outliers that do not fit your data distribution. In the next course, you will learn more methods of visualizing a data set to help you quickly identify and remove outliers.\n",
    "\n",
    "* **Binning:** This method refers to categorizing ranges of data in logical bins or groups - for example: low, medium, or high - to make it more meaningful for your analysis.\n",
    "\n",
    "* **Variable Transformation:** In the next course, you will learn the various distributions that may describe your data set. Typically, we want our data to be normally distributed which makes it easier to analyze. In the cases where data is not normally distributed, there are a variety of methods that can be applied to correct for this, including logarithmic, Box-Cox or exponential transformations.\n",
    "\n",
    "* **Feature Creation:** You can use a variety of mathematical functions to create new features. In the house price example mentioned above, we combined two columns to create a more meaningful data point. You can use this method, along with addition, subtraction, mean calculation, min/max, product or any other relevant methods that can create a more meaningful variable.\n",
    "\n",
    "These are just some common methods of feature engineering. The concept is both a science and an art, in that you can choose the method that makes the most sense for the objectives of your model.\n",
    "\n",
    "In Python, you can use NumPy and Pandas for most of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, we can use both quantitative (numerical) or qualitative (categorical) data to predict outcomes. In the last module, we learned about linear regression models. These models work easily with numerical data, however it is more difficult to use them for categorical data. For example, let's consider that we want to predict the political vote of a group. The attributes \"Republican\", \"Democrat\" and \"Independent\" are categorical variables. In order to use them in a regression, or machine learning model, we would need to transform them into a numerical value. This is where we introduce Dummy Variables.\n",
    "\n",
    "Dummy variables take the form of 0 or 1 and are used to represent mutually exclusive categories in your analysis - for example, Repubican vs. Other. \n",
    "\n",
    "The number of dummy variables you should use depends on the number of categories a value can assume. For example, if you would like to categorize political affiliation, you may set your dummy variables as follows:\n",
    "\n",
    "$ x_{1} = 1,\\ if \\ Republican,\\ and\\ x_{1} = 0,\\ if\\ Otherwise \\\\\n",
    " x_{2} = 1,\\ if\\ Democrat,\\ and\\ x_{2} = 0,\\ if\\ Otherwise \\\\\n",
    " x_{3} = 1,\\ if\\ Independent,\\ and\\ x_{3} = 0,\\ if\\ Otherwise $\n",
    "\n",
    "However, you should be cautious about the \"Dummy Variable Trap\". This refers to defining too many dummy variables and causing a multicollinearity problem in your model. Multicollinearity occurs when two or more independent variables are related to each other, and this can impact the accuracy of your results. For example, let's consider the case of Smoker vs. Non-Smoker. If you were to assign dummy variables to both, such that:\n",
    "\n",
    "$ x_{1} = 1,\\ if \\ Smoker,\\ and\\ x_{1} = 0,\\ if\\ Non-Smoker \\\\\n",
    " x_{2} = 1,\\ if\\ Non-Smoker,\\ and\\ x_{2} = 0,\\ if\\ Smoker $\n",
    "\n",
    "Including both of these variables in a model would be redundant, because if we know that someone is a \"Smoker\", that automatically means they are NOT a \"Non-Smoker\". Thus, we only need one variable.\n",
    "\n",
    "In Python, you can create dummy variables by using Pandas. The function is pd.get_dummies(df['column name']) and you can merge the dummy columns with your existing data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models can be used to predict outcomes. We build them using an existing data set, and typically apply them to new data sets once complete. But, how do we know if the prediction will be accurate when applied to a new data set? What if the existing data was biased, and we didn't realize?\n",
    "\n",
    "Well, this is where the concept of **cross-validation** comes in. The purpose of cross-validation is to ensure that the model we build is accurate for existing/known data, but also for independent or new data. There are several methods of cross-validation, but for purposes of this class we will focus on the simplest method: splitting your original data set into a **training** data set and a **testing** data set.\n",
    "\n",
    "The **training** subset is used to train the model and fit the model parameters.\n",
    "The **test** subset is used to test how well your model performs on the new data.\n",
    "\n",
    "Why do we need to split the data into two subsets? Let's imagine that we created a model and fine-tuned the parameters so well that the accuracy of the model is almost 100% and it perfectly fits the data which we have. What will be the accuracy of the model on a new data? Will the model work for the data which was not used to train the model? The accuracy of the model can only be determined by considering how well a model performs on new data that was not used when fitting the model.\n",
    "\n",
    "Below are two examples when the model is **overfitted**, meaning that the model performs very well on the training data but it does not generalize well. Both examples are used in [this Wikipedia article](https://en.wikipedia.org/wiki/Overfitting) on overfitting.\n",
    "\n",
    "**Example 1:** The green line represents an overfitted classification model and the black line represents a better generalized model. While the green line perfectly describes the training data, it is likely to have a high error rate on new data, compared to the black line.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/19/Overfitting.svg' height=\"300\" width=\"300\" alt=\"Overfitted classification model (green line)\">\n",
    "\n",
    "**Image source:** Image by [Chabacano - Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=3610704)\n",
    "\n",
    "**Example 2:** In this example, noisy data is perfectly fitted to a polynomial function (blue line). Even though the polynomial function provides a perfect fit, it won't perform well on a new data. The simple linear function would be a better fit for this data.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/6/68/Overfitted_Data.png' height=\"400\" width=\"400\" alt=\"Overfitted model\">\n",
    "\n",
    "**Image source:** Image by [Ghiles - Own work, CC BY-SA 4.0](https://commons.wikimedia.org/w/index.php?curid=47471056)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, the original dataset is split 70/30 or 80/20. This means that 70% (or 80%) of all data points are used to  train the model and the remaining data is reserved for testing.\n",
    "\n",
    "Each model trained on the train subset can be tested on the test subset to see its predictive performance. This allows fine tuning of the prediction model.  \n",
    "\n",
    "As you progress through the certificate, you will learn other methods of cross-validation, including K-fold and repeated random sub-sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Other Modeling Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of other modeling techniques which you will learn in later courses of this certificate. These include:\n",
    "\n",
    "* **Logistic Regression:** this method is largely used for classification problems, and this is where the concept of dummy variables applies. For example, you can also use a logistic regression to predict explosions due to o-ring damage from our Challenger problem (e.g. 1 = damaged, 0 = not damaged). Logistic regression models compute the probability that an instance belongs to a positive class (e.g. 1 = damaged), and makes a placement decision based on a defined threshold value. You can use the scikit-learn library within Python for this type of regression. Below is an example of the output.\n",
    "\n",
    "<img src='DmgTempLogReg.png' height=\"400\" width=\"400\" alt=\"Logistic Regression\">\n",
    "\n",
    "* **Support Vector Machines (SVM):** SVM algorithms are very powerful and versatile, they can be applied to both classification and regression problems, and are used often for outlier detection. They are not limited to linear problems, and are capable of performing non-linear classification and regression.\n",
    "\n",
    "<img src='SVM.png' height=\"500\" width=\"500\" alt=\"SVM\">\n",
    "\n",
    "* **Decision Trees:** Decision Trees is a special family of Machine Learning algorithms. These algorithms are versatile and can perform both classification and regression tasks. Essentially, they break down data sets into smaller and smaller data sets until a decision is reached. They are very powerful, can be used to fit complex datasets but also they are used as a component of **Random Forests** algorithms, which are among the most powerful Machine Learning algorithms available today.\n",
    "\n",
    "<img src='DecisionTree.png' height=\"500\" width=\"500\" alt=\"Decision Tree\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this certificate, in the Machine Learning course you will learn how to build the aforementioned types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "You have reached the end of this module. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connor Johnson blog (http://connor-johnson.com/2014/02/18/linear-regression-with-python/).\n",
    "\n",
    "MNIST database (https://en.wikipedia.org/wiki/MNIST_database).\n",
    "\n",
    "Ng, A. (2018)  _Machine Learning Yearning_ (electronic book).\n",
    "\n",
    "Unsupervised Learning. (https://en.wikipedia.org/wiki/Unsupervised_learning#Approaches).\n",
    "\n",
    "Witten, I.H, Frank, E. (2005) *Data Mining. Practical Machine Learnng Tools and Techniques* (2nd edition). Elsevier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
