{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Data Collection & Cleaning Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Collecting and cleaning data is the key topic discussed in this module.\n",
    "We will continue working with `pandas`, focusing on dealing with dirty data, and go through the process with the most common use case of web data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "In this module, you will do the following using Python libraries for gathering and preparing data.\n",
    "\n",
    "* Discuss types of data\n",
    "* Get data from the web\n",
    "* Clean up problem data using `pandas`\n",
    "* Handle missing data\n",
    " \n",
    "Our emphasis will be on extraction, transformation and loading methods. This module consists of 3 parts.\n",
    "\n",
    "* Part 1 - Data Sources\n",
    "* Part 2 - Web Scraping\n",
    "* Part 3 - Data Preparation\n",
    "\n",
    "Each part is provided in a separate file. It is recommended that you follow the order of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readings and Resources\n",
    "\n",
    "The majority of the notebook content borrows from the recommended readings. We invite you to further supplement this notebook with the following recommended texts.\n",
    "\n",
    "McKinney, W. (2017). Python for data analysis: Data wrangling with Pandas, NumPy, and IPython (2nd Ed.). O'Reilly Media.\n",
    "  * Ch. 6 (p 167-178, 186-187)\n",
    "  * Ch. 7 (p. 191-213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Module-5:-Data-Collection-&amp;-Cleaning\" data-toc-modified-id=\"Module-5:-Data-Collection-&amp;-Cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Module 5: Data Collection &amp; Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Learning Outcomes</a></span></li><li><span><a href=\"#Readings-and-Resources\" data-toc-modified-id=\"Readings-and-Resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Readings and Resources</a></span></li><li><span><a href=\"#Preparing-the-Data:-Importing-and-Cleaning-Data\" data-toc-modified-id=\"Preparing-the-Data:-Importing-and-Cleaning-Data-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Preparing the Data: Importing and Cleaning Data</a></span></li><li><span><a href=\"#Data-Sources\" data-toc-modified-id=\"Data-Sources-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Data Sources</a></span><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Websites\" data-toc-modified-id=\"Types-of-Websites-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Types of Websites</a></span></li><li><span><a href=\"#Web-Page-Contents\" data-toc-modified-id=\"Web-Page-Contents-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Web Page Contents</a></span><ul class=\"toc-item\"><li><span><a href=\"#HTML\" data-toc-modified-id=\"HTML-1.5.2.1\"><span class=\"toc-item-num\">1.5.2.1&nbsp;&nbsp;</span>HTML</a></span></li><li><span><a href=\"#XML\" data-toc-modified-id=\"XML-1.5.2.2\"><span class=\"toc-item-num\">1.5.2.2&nbsp;&nbsp;</span>XML</a></span></li><li><span><a href=\"#CSS\" data-toc-modified-id=\"CSS-1.5.2.3\"><span class=\"toc-item-num\">1.5.2.3&nbsp;&nbsp;</span>CSS</a></span></li><li><span><a href=\"#JavaScript\" data-toc-modified-id=\"JavaScript-1.5.2.4\"><span class=\"toc-item-num\">1.5.2.4&nbsp;&nbsp;</span>JavaScript</a></span></li><li><span><a href=\"#Semantic-Web-Content\" data-toc-modified-id=\"Semantic-Web-Content-1.5.2.5\"><span class=\"toc-item-num\">1.5.2.5&nbsp;&nbsp;</span>Semantic Web Content</a></span></li><li><span><a href=\"#Images\" data-toc-modified-id=\"Images-1.5.2.6\"><span class=\"toc-item-num\">1.5.2.6&nbsp;&nbsp;</span>Images</a></span></li><li><span><a href=\"#Micro-formats\" data-toc-modified-id=\"Micro-formats-1.5.2.7\"><span class=\"toc-item-num\">1.5.2.7&nbsp;&nbsp;</span>Micro-formats</a></span></li></ul></li></ul></li><li><span><a href=\"#Links\" data-toc-modified-id=\"Links-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Links</a></span></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data: Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"dataScienceProcess.png\" alt=\"This image shows the stages of creating a model from start to finish.\" style=\"width: 100%;\"> (Course Authors, 2018)\n",
    "    <figcaption><em>This image shows the stages of creating a model from start to finish.</em></figcaption>\n",
    "</figure>\n",
    "\n",
    "Now that you have learned some basic functions of Python, we will begin exploring real data sources. When you clearly define your business problem, you should consider what data may be required to solve the problem. During this phase of the analytics methodology, you should - within the boundaries of your organizational policies - collect all data points that you think are relevant to your problem. This includes internal or external data.\n",
    "\n",
    "Once you have the data points that may be relevant, you will conduct preliminary reviews of the data to answer some of the following questions: is the data set complete? Is it accurate? Reliable? Unbiased? Is there enough data to draw significant conclusions about my problem? You can do this by leveraging basic functions in NumPy and Pandas to organize and understand your data set.\n",
    "\n",
    "Once you have narrowed down the data points you need, the next step is to cleanse the data such that it is consistent and usable and in your analysis. In this module, you will first learn how to access data from a variety of external data sources (for example, websites), and then how to clean the data you have selected to solve your problem. Once your data is clean, you will be able to move on to the analysis and model development phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, many websites have made available public Application Programming Interfaces (APIs) providing data feeds (Motto, 2018). Web and Software development standards have forced a shift in the way these API's deliver data. Originally there was no standard, but with the formation of the World Wide Web Consortium (W3C) (W3C, 2018) many web technologies became standardized. \n",
    "\n",
    "The first major shift in web development practices came with Web 2.0 (Web 2.0, nd), where web client-side technologies (* i.e., what runs in your web browser*) were separated by concerns. What was originally only HyperText Markup Language (HTML)(WC3 Community, 2018) was split off into three main technologies.\n",
    "\n",
    "* **Extensible Markup Language (XML)** (W3C Community, 2016)(*i.e., content and metadata*)\n",
    "* **Style Sheets** (Bos, 2018) (*i.e., for typography or formatting. *)\n",
    "  * The most well recognized technology being **Cascading Style Sheets (CSS)** (Mozilla and individual contributors, 2018)\n",
    "* **Client-Side JavaScript** (HazaÃ«l-Massieux, 2016)\n",
    "\n",
    "The second major shift came in the mid 2000's, when web technologies began standardization for Web 3.0 (*a.k.a. The Semantic Web*)(Semantic Web, nd). The main distinction between Web 3.0 and 2.0 is due to separating content (i.e. XML) from data (i.e. RDF, OWL, JSON-LD). The main technologies of Web 3.0 allow resources to be fully queryable. In other words, the shift to the semantic-web is what helped push the technology sector to become data-driven. Unfortunately, the semantic web did not pan out as once intended. The original vision of the semantic web was to decentralize data and treat the internet as a database. While the technology is in use by major companies (Schema.org community, 2018), cloud-computing and Big-Data technologies alleviated many of the problems the semantic web aimed to solve. As of 2018, the major use of semantic web technologies is to leverage wikis to become sources of data. While this may not seem like an impressive use case, it is the necessary technology that drives IBM Watson, which was originally a centralized data source reliant entirely on querying semantic-web-enabled wikis to further increment its knowledge graph, and to also intelligently query the graph to answer questions. As of 2018, the semantic-web stack and its use in IBM Watson is the most impressive example of automated *Data Cleaning* to date.\n",
    "\n",
    "A similar trend has also occurred for web applications. Web 2.0 was the standard distinguishing web applications from  regular websites. The emergence of web applications gave a new way to deliver Software-As-A-Service (SaaS) (Software as a Service, nd), web applications now being the most popular delivery method. With the arrival of cloud-computing, SaaS became the standard for delivering information technology products. In this process, distribution of software became an increasing concern, leading to adoption of decentralized software architectures. The main pattern in use as of 2018 is that of Micro-Services (Microservices, nd) where application processes are kept as independent as possible from one another, resulting in minimal cascading of failures during downtime, and easy replication and recovery of services. As such, data feeds are often given their own separate Application Programming Interface (API) (Application Programming Interface, nd) so as to separate concerns more easily (*i.e. querying for data is separated from processing data*). The the consequence of this design results in large amounts of network traffic over the Internet to communicate between these largely independent and often replicated application processes. As of 2018, the optimal message format that reduces network traffic between application processes is JavaScript Object Notation (JSON) (Bray, 2017).\n",
    "\n",
    "In this section, we focus on scraping from web pages and resources, and also accessing web APIs. As data sources, web APIs have become the de-facto querying method for the majority of enterprises with which to both deliver services and consume data feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we focus on extraction of data from an online data source. \n",
    "\n",
    "Data sources show a major distinction in terms of data with respect to *accessibility*. \n",
    "Internal data sources are more easily accessible, and don't require respecting special copyright or intellectual property laws. Internal data sources tend to be personal or data generated by an agent or organization individually. External data sources tend to be data shared by others collectively for profiling or aggregation purposes.\n",
    "\n",
    "| **Internal**               | **External**                                  |\n",
    "| ---: | --- |\n",
    "| Transactional              | Financial markets                             |\n",
    "| Systems health             | Events and news feeds                         |\n",
    "| Financial                  | Social media (Twitter, LinkedIn, Facebook)    |\n",
    "| Concepts / classifications | Location-based (cellphones, tracking devices) |\n",
    "| Documents or other text    | Social and economic databases                 |\n",
    "| Email                      | Open Data                                     |\n",
    "| Devices                    | 3rd-party data vendors                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the type of resource used for holding data are generally the same for both external and internal. As such we will focus on external data sources, the majority of which are made up of web resources. There are three different variants of resources.\n",
    "\n",
    "| Web Resource Variant | Description |\n",
    "| --- | --- |\n",
    "| Static pages | Static pages refers to web resources that stay the same after being initially loaded. * i.e. html files or any file resource such as csv, excel, pdf, etc. * |\n",
    "| Dynamic pages | These refer to pages with server-side or client-side code actively making changes after the initial page resources have been loaded. * i.e. pages changed by dynamic javascript, or web application pages running server-side code.* |\n",
    "| APIs | These refer to interfaces for accessing data housed in applications. In recent years these have been standardized to use Javascript Object Notation (JSON). |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:53:30.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup code and importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.options.display.max_rows = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways we can extract these resources. One is to extract the data manually (static and dynamic pages). The second is to use a provided interface (APIs, but could apply to all three). We will be focusing mainly on manual extraction in this section.\n",
    "\n",
    "First, we can use a library to access the resource and make a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:53:38.443Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py:986: InsecureRequestWarning: Unverified HTTPS request is being made to host 'raw.githubusercontent.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Code,Name\\nAF,Afghanistan\\nAX,Ãland Islands\\nAL,Albania\\nDZ,Algeria\\nAS,American Samoa\\nAD,Andorra\\nAO,Angola\\nAI,Anguilla\\nAQ,Antarctica\\nAG,Antigua '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get('https://raw.githubusercontent.com/openmundi/world.csv/master/countries(249)_alpha2.csv', verify=False)\n",
    "r.text[0:140]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the data into `pandas` using a **buffer** instead of a **file descriptor**. A *buffer* is simply an array of memory and *file descriptor* is a buffer that holds descriptive information about a file, but not the actual contents (i.e., the file's location, or where in the file the current file reading buffers are positioned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:53:51.294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AF</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AX</td>\n",
       "      <td>Ãland Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>Albania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>YE</td>\n",
       "      <td>Yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>ZM</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code           Name\n",
       "0     AF    Afghanistan\n",
       "1     AX  Ãland Islands\n",
       "2     AL        Albania\n",
       "..   ...            ...\n",
       "246   YE          Yemen\n",
       "247   ZM         Zambia\n",
       "248   ZW       Zimbabwe\n",
       "\n",
       "[249 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "#read in request as if it was a file, but really a buffer\n",
    "pd.read_csv(filepath_or_buffer=io.StringIO(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about when the data requested is more complicated? What if the request is to a markup file, like HTML? Or an API?\n",
    "Many web APIs will return a JSON string that must be loaded into a Python object. How do we convert the JSON into something usable, like a `pandas` `DataFrame`?\n",
    "\n",
    "*This is where `pandas` begins to truly shine*. There are read functions in `pandas` for handling JSON and HTML. The function for handling HTML is implemented using more general libraries, meaning it actually attempts to handle  XML structure first, before falling back on HTML parsing. \n",
    "\n",
    "Both parsing methods handle multi-indexing for nested markup and return lists of type `DataFrame` for HTML with multiple tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:53:21.295Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jul 1, 2018</th>\n",
       "      <td>2793.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jun 1, 2018</th>\n",
       "      <td>2754.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May 1, 2018</th>\n",
       "      <td>2701.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mar 1, 1871</th>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feb 1, 1871</th>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jan 1, 1871</th>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1770 rows Ã 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Price Value\n",
       "Date                    \n",
       "Jul 1, 2018      2793.64\n",
       "Jun 1, 2018      2754.35\n",
       "May 1, 2018      2701.49\n",
       "...                  ...\n",
       "Mar 1, 1871         4.61\n",
       "Feb 1, 1871         4.50\n",
       "Jan 1, 1871         4.44\n",
       "\n",
       "[1770 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.read_html(\"http://www.multpl.com/s-p-500-historical-prices/table/by-month\",\n",
    "                 header=0,\n",
    "                 skiprows=[1, 16],\n",
    "                 index_col=0)\n",
    "#**NOTE**: A *list* of `DataFrames` is returned\n",
    "scrapedTable = x[0] \n",
    "scrapedTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:02.036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Price Value'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapedTable.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are still restrictions on `read_html()` , like only extracting `<table>` elements. What if another elements holds the data? Or, if the data is sporadically embedded?\n",
    "\n",
    "Thus, we'll cover how to extract data in general from these resources. But first a closer look at how our web resource contents are structured and organized.\n",
    "\n",
    "### Web Page Contents\n",
    "\n",
    "We begin by refreshing ourselves on the structure of each data source resource type that might be encountered and their intended use. We'll go over the most prevalent content types and their characteristics.\n",
    "\n",
    "#### HTML\n",
    "\n",
    "**Hyper Text Markup Language (HTML)** (WHATWG Apple, Google, Mozilla, Microsoft, 2018) has undergone many changes since its first inception. The intent behind HTML is for **presenting** structured content. It is quite literally a language for specifying the displaying of content. As of 2018, HTML is ***not*** valid XML. If you held a misconception that HTML was valid XML, then be at ease. That was the goal of their respective W3C standardization committees. This is what lead to XML-valid HTML spinning-off into the X/HTML standard. However, this standard's future is uncertain due to poor adoption.\n",
    "\n",
    "In general, it is simpler to assume HTML has the same structure as XML.\n",
    "\n",
    "#### XML\n",
    "\n",
    "XML consists of text, attributes, elements and special characters.\n",
    "```\n",
    "<rootElement>\n",
    "  <childElement anAttribute=\"aValue\">\n",
    "    aTextValue\n",
    "    <leafElement> anotherTextValue </leafElement>\n",
    "    <emptyleafElement withAnAttribute=\"anotherValue\" />\n",
    "    \n",
    "  </childElement>\n",
    "</rootElement>\n",
    "```\n",
    "Unlike HTML, XML has ***no*** defined elements. It is used to define other file formats. As such, XML has two ways to be evaluated.\n",
    "\n",
    "| Evaluation Type | Description |\n",
    "| --- | --- |\n",
    "| Validity | If the XML in question is well formed ***and*** also adheres to a definition or schema, like XHTML, then it is valid XML. |\n",
    "| WellFormedness | If elements are strictly nested as a tree format and there is no ambiguity of what is an attribute, element, and text, then the file is well-formed XML. Definitions and schemas are not required. | \n",
    "\n",
    "Validity is determined by some sort of schema or data definition. This is usually in the form of a Data Type Definition (DTD) file or an XSchema file. Both allows for grammars to be defined for the XML file, but DTD defines a context-free grammar whereas XSchema defines a context-sensitive grammar (more expressive) . In order for an XML document to be well-formed, special characters are necessary. For example, how do you express a mathematical inequality without introducing a new element?\n",
    "\n",
    "| Before special characters | After special characters |\n",
    "| --- | --- |\n",
    "| ```<anInEquality>1<x</anInEquality>``` | ```<anInEquality>1&lt;x</anInEquality>``` |\n",
    "\n",
    "Similarly, how can you specify a special character without specifying a special character? If you've ever wondered why `&` throw errors when editing raw XML and HTML, now you know why. It is a character reserved for denoting special characters.\n",
    "\n",
    "**NOTE**: It is possible to parse malformed XML from a theoretical perspective. But, it's terribly inefficient and would slow your browser to a crawl if allowed in large one-page web applications. More precisely, XML can be parsed using a context-free grammar. HTML cannot be parsed in the same manner and requires a context-sensitive grammar, which is a more expressive grammar that requires more computation to execute its algorithm.\n",
    "\n",
    "See [Semantic Web Content](#Semantic-Web-Content) for more examples.\n",
    "\n",
    "#### CSS\n",
    "\n",
    "**Cascading Style Sheets (CSS)** (Mozilla and individual contributors, 2018) have a simple key-value pair like format. Properties are set to values for every instance of an element selection. Element selections (a.k.a. selectors) are simply used to match on branches of HTML or XML.\n",
    "\n",
    "```\n",
    "<selector1> [, <selector2>, ...] {\n",
    "  property11: <value111> [, <value112>, ...];\n",
    "  ...\n",
    "  property1N: <value1N1> [, <value1N2>, ...];\n",
    "}\n",
    "...\n",
    "<selectorM1> [, <selectorM2>, ...] {\n",
    "  propertyM1: <valueM11> [, <valueM12>, ...];\n",
    "  ...\n",
    "  propertyMN: <valueMN1> [, <valueMN2>, ...];\n",
    "}\n",
    "```\n",
    "The following is an example snippet of CSS. \n",
    "\n",
    "```\n",
    "body {\n",
    "  colour: blue;\n",
    "  background-color: yellow;\n",
    "  border: 1px solid black;\n",
    "}\n",
    "```\n",
    "\n",
    "The snippet simply matches on a header element, similarly to how a *regular expression* might match on a substring. Upon a successful match, it does the following.\n",
    "\n",
    "* Sets the element colour property to blue\n",
    "* Sets the background colour for the displayed text to yellow\n",
    "* Sets a 1 pixel thick solid (as opposed to broken or dashed) black border surrounding the element's displayed text.\n",
    "\n",
    "The rule then continues to attempt matching on another such element further in the document.\n",
    "\n",
    "At this point, you might wonder why CSS exists. Isn't it doing the same thing as HTML by deciding how a page will be presented? To an extent, this is the case. However, XML with CSS is not capable of all the display options as HTML is alone. HTML actually has a superset of capabilities. However, CSS does allow for different CSS style sheets to be used for different display mediums (i.e., phone vs. tablet vs. computer vs. TV vs. projector vs. , etc.) in a clean and modular fashion. The equivalent of this in one HTML page would be a mess of style injections for different elements, and prone to breaking depending on browser support.\n",
    "\n",
    "The existence overlap of CSS with HTML brings forth the primary motivation for moving away from HTML. HTML is not maintainable for complex logic in an environment where web browsers may not support standards consistently. In terms of web scraping, CSS forces a decoupling of content and style, making the it easier to scrape and clean content.\n",
    "\n",
    "#### JavaScript\n",
    "\n",
    "JavaScript is a programming language which enables interactive web content. It is often used for communicating with web applications and acts as the primary programming layer for all client-side requests. Javascript enables content and the display of pages to change during the time a webpage is view. Its main use is for enabling User Interface logic, but the language is a full fledged programming language and has the same general capabilities as any other programming language.\n",
    "\n",
    "However, Javascript is more standardized in terms of client-side web applications. In a browser environment, Javascript makes use of the **Document Object Model (DOM)**. The DOM is a data structure that represents the current state of the web page content, and contains all information and programming event logic associated with CSS and HTML/XML (i.e., url, window dimensions, html used for rendering, HTML currently displaying, previously visited URL, etc.). To put it in layman terms, the CSS and HTML/XML/XHTML content are all manifested by the web browser into the DOM, which the browser then manipulates using Javascript. It would be accurate to say the DOM is the active state of the web page. With respect to the DOM, Javascript manipulations are forced to only trigger event listeners (like a mouse click) attached on elements, and then run a handler/function for reacting to listened events (like the effect of clicking a button). \n",
    "\n",
    "Events are always listened to in the direction from the triggered element up to the root element. Events are handled in the reverse direction. This is event handlers.\n",
    "\n",
    "For our purposes, we are more interested in how JavaScript is used for communicating with web applications, as this is the use case for querying data through a web API. Since requests between servers and clients are simply packets of text or binary information, Javascript can be directly injected into the content of either a request or a response. Since the browser already uses a DOM to represent all client-side data, it is often easier to use JavaScript values and objects directly to manipulate it. Hence, the JavaScript Object Notation (JSON) is used directly to transmit message content, instead of some special messaging protocol. The primary motivation for letting the server send injectable-code (usually considered a security risk) as data is simple. The client already trusts the server, otherwise it wouldn't have connected to the URL. This is a good assumption if your web browser is its own separate process and cannot manipulate your computer's operating system (i.e., MS Internet Explorer for older versions of the Windows operating system was integrated into the operating system, resulting in javascript code capable of shutting down computers or forcing file downloads).\n",
    "\n",
    "\n",
    "JSON format is similar to CSS in that it is a key-value pair. Unlike a simple key-value pairs, JavaScript values can be arrays and objects, and not only primary values, like strings, booleans, and numbers.\n",
    "\n",
    "```\n",
    "{\n",
    "  \"property1\": \"stringValue\",\n",
    "  \"property2\": true,\n",
    "  \"property3\": 0,\n",
    "  \"property4\": {\n",
    "    \"embeddedJSONProperty\": \"embeddedJSONValue\",\n",
    "  },\n",
    "  \"property5\": [\n",
    "    {\n",
    "      \"arrayOfEmbeddedJSONProperty1\": \"value1\",\n",
    "    },\n",
    "    {\n",
    "      \"arrayOfEmbeddedJSONProperty2\": \"value2\",\n",
    "    }\n",
    "  ],\n",
    "  \"property6\": [],\n",
    "  \"property7\": null\n",
    "}\n",
    "```\n",
    "\n",
    "See [Semantic Web Content](#Semantic-Web-Content) for more examples.\n",
    "\n",
    "#### Semantic Web Content\n",
    "\n",
    "Semantic web markup is used to describe Entities. Entities can have the following attached to them.\n",
    "\n",
    "* relationships\n",
    "* attributes\n",
    "* types\n",
    "\n",
    "This is very similar to a database. However, semantic web markup is meant for describing resources so as to make data queryable. The major file formats used to describe semantic web resources are in the following table.\n",
    "\n",
    "| Format | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Resource Description Framework (RDF) | RDF is the most minimal of those mentioned. RDF is an XML format which makes full use of XSchema type values. It allows class relationships, and instance relationships to be defined along with properties and attributes. | see 1 |\n",
    "| Web Ontology Language (OWL) | OWL is more expressive than RDF. OWL is an XML format which makes full use of XSchema type values. OWL is actually Turing-complete in terms of expressiveness. Additionally, unlike RDF, OWL has an all encompassing class a null class. | see 2 |\n",
    "| Turtle (TTL) | TTL is a non tabular record format. It is actually just a different way of writing data. TTL is not a document type like XML or HTML. In fact, it is usually defined using either RDF or OWL. | see 3 |\n",
    "| N-Triple | N-Triple is a tabular record format. It is a different way of writing data. N-Triple is not a document type like XML or HTML. In fact, it is usually defined using either RDF or OWL. | see 4 |\n",
    "| JavaScript Object Notation for Linked Data (JSON-LD) | Since JSON has a tree structure like XML, RDF and OWL data is also expressible. The only difference is that a `type` and `id ` must be specified for each resource. | see 5 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.```<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<rdf:RDF xmlns:contact=\"http://www.w3.org/2000/10/swap/pim/contact#\" xmlns:eric=\"http://www.w3.org/People/EM/contact#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
    "  <rdf:Description rdf:about=\"http://www.w3.org/People/EM/contact#me\">\n",
    "    <contact:fullName>Eric Miller</contact:fullName>\n",
    "  </rdf:Description>\n",
    "  <rdf:Description rdf:about=\"http://www.w3.org/People/EM/contact#me\">\n",
    "    <contact:mailbox rdf:resource=\"mailto:e.miller123(at)example\"/>\n",
    "  </rdf:Description>\n",
    "  <rdf:Description rdf:about=\"http://www.w3.org/People/EM/contact#me\">\n",
    "    <contact:personalTitle>Dr.</contact:personalTitle>\n",
    "  </rdf:Description>\n",
    "  <rdf:Description rdf:about=\"http://www.w3.org/People/EM/contact#me\">\n",
    "    <rdf:type rdf:resource=\"http://www.w3.org/2000/10/swap/pim/contact#Person\"/>\n",
    "  </rdf:Description>\n",
    "</rdf:RDF>``` \n",
    "\n",
    "2.```<rdf:RDF ...> <owl:Ontology rdf:about=\"\"/> <owl:Class rdf:about=\"#Tea\"/> </rdf:RDF>``` \\cite{2018a}\n",
    "\n",
    "3.```@prefix eric:    <http://www.w3.org/People/EM/contact#> .\n",
    "@prefix contact: <http://www.w3.org/2000/10/swap/pim/contact#> .\n",
    "@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "eric:me contact:fullName \"Eric Miller\" .\n",
    "eric:me contact:mailbox <mailto:e.miller123(at)example> .\n",
    "eric:me contact:personalTitle \"Dr.\" .\n",
    "eric:me rdf:type contact:Person .``` \n",
    "\n",
    "4.```<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#fullName> \"Eric Miller\" .\n",
    "<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#mailbox> <mailto:e.miller123(at)example> .\n",
    "<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#personalTitle> \"Dr.\" .\n",
    "<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2000/10/swap/pim/contact#Person> .``` \n",
    "\n",
    "5.``` \n",
    "  {\"@context\": {\n",
    "    \"name\": \"http://xmlns.com/foaf/0.1/name\",\n",
    "    \"homepage\": {\n",
    "      \"@id\": \"http://xmlns.com/foaf/0.1/workplaceHomepage\",\n",
    "      \"@type\": \"@id\"\n",
    "    },\n",
    "    \"Person\": \"http://xmlns.com/foaf/0.1/Person\"\n",
    "  },\n",
    "  \"@id\": \"http://me.example.com\",\n",
    "  \"@type\": \"Person\",\n",
    "  \"name\": \"John Smith\",\n",
    "  \"homepage\": \"http://www.example.com/\"}``` \n",
    "\n",
    "This numerous formats are the consequence of enabling different data sources to be linked, regardless of format. But, the important thing to remember is that the semantic markup can contain data, or schema typing information, or complex value definitions. As such, the semantic web is only as useful as the browser makes it. In the case of semantic web sources, they are most useful when scraping data without having to clean it.\n",
    "\n",
    "#### Images\n",
    "\n",
    "Images generally come in two formats.\n",
    "\n",
    "| General Formats | Description |\n",
    "| --- | --- |\n",
    "| Vector | This format stores all points and lines as geometric data. As a result scaling the image up results in no loss of resolution. Instead the image is simply re-rendered. |\n",
    "| Raster | This format stores all pixels individually as binary data. As a result, the image loses resolution as it is scaled up. However, it doesn't require a fresh computation for every rescale. |\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/aa/VectorBitmapExample.svg\" alt=\"Vector vs Bitmap\" style=\"width: 200px;\"/>\n",
    " Image Source: ``_File:vectorbitmapexample.svg --- wikimedia commons, the free media repository_'',  2018.  [online](https://commons.wikimedia.org/w/index.php?title=File:VectorBitmapExample.svg&oldid=294435857)\n",
    "\n",
    "In general, these images can be stored in any file format, but with the shift towards the semantic web, more vector formats tend to be XML based. The reasoning is actually related to standardization and committees spinning off from the W3C. In particular, there is a specific push by the Web3D Consortium (Web 3D Consortium, 2018) to be able to render 3D data and visualizations in a way that 3D models can be re-used by Computer Assisted Design (CAD) programs. For example, a geographic map is actually a 2D projection of a 3D model of the earth. As such, if the vector image of a map also contains all this data and renders from scratch starting with the 3D data, then this image can be reused by other geographers for other maps.\n",
    "\n",
    "\n",
    "While this might seem like a rare use case, the cost to make maps using satellite raster images is quite expensive depending on the resolution. Similarly, the model of the earth is not fixed. As a result, there is a demand to be able to reuse information, even if it isn't raw data but models interpreting the data, such as 3D mathematical models of the earth.\n",
    "\n",
    "#### Micro-formats\n",
    "\n",
    "Micro-formats are loosely defined, but in general they are formats embedded in other resources. \n",
    "Let us consider Wikipedia. Many pages in Wikipedia have info-boxes via table summarizations (resource description framework, nd.). These table summarization, though defined and rendered in HTML, are constrained by the attributes used to identify them. And this constraint is consistent across the entirety of Wikipedia, because they are actually standardized and enforced by the authors themselves. Their intended use is to provide hooks into content to extract data. As such, the micro-formats are very minimal and only require annotating class information and relationships between classes on to the format.\n",
    " \n",
    "| Example of Data | Example of Data with Micro-format |\n",
    "| --- | --- |\n",
    "| ```<ul><li>Joe Doe</li><li>The Example Company</li><li>604-555-1234</li><li><a href=\"http://example.com/\">http://example.com/</a></li></ul>``` | ```<ul class=\"vcard\"><li class=\"fn\">Joe Doe</li><li class=\"org\">The Example Company</li><li class=\"tel\">604-555-1234</li><li><a class=\"url\" href=\"http://example.com/\">http://example.com/</a></li></ul>``` |\n",
    "\n",
    "\n",
    "The use of this information is usually for easier extraction and usually to link up with semantic web data. An example of this is GeoNames (GeoNames commnity, 2013) which extracts from multiple sources, but makes use of the micro-formats for all place-taxonomy Wikipedia pages that also have geographic coordinate systems. As a result the data (extracted from publicly available resources) is freely available.\n",
    "\n",
    "As of 2018, Micro-formats have a standard being recommended as part of the HTML standard.\n",
    "\n",
    "## End of Part 1\n",
    "\n",
    "This notebook makes up one part of this module. Now that you have completed this part, please proceed to the next notebook in this module.\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links\n",
    "\n",
    "* http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python\n",
    "  * About scraping using other python libraries, as well as crawling entire websites.\n",
    "* http://scrapy.org/\n",
    "  * About writing scrapers as configeration files via scrapy.\n",
    "* https://docs.python.org/2/library/urllib2.html\n",
    "  * documentation for urlib2 library\n",
    "* http://docs.python-requests.org/en/latest/\n",
    "* The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)\n",
    "  * https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "* http://import.io\n",
    "  * a web-based platform for extracting data from websites without writing any code.\n",
    "* http://www.crummy.com/software/BeautifulSoup/\n",
    "  * popular alternative to lxml for web/screen scraping\n",
    "* http://pbpython.com/web-scraping-mn-budget.html\n",
    "  * Tutorial using BeautifulSoup with requests library, pandas, numpy and mathplotlib\n",
    "* Python Regular Expressions Cheat Sheet\n",
    "  * https://pycon2016.regex.training/cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Application programming interface, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Application_programming_interface&oldid=848903375)\n",
    "\n",
    "Bos, B. (2018). ``_Web style sheets_''.  [online](https://www.w3.org/Style/)\n",
    "\n",
    "Bray, 2017. The javascript object notation data interchange format. [online](https://www.rfc-editor.org/rfc/rfc8259.txt)GeoNames community, 2013. GeoNames. [online](http://www.geonames.org/)\n",
    "\n",
    "C. S. S. working group, 2018. ``_Cascading style sheets_'', Retrieved July 2018.  [online](https://www.w3.org/Style/CSS/)\n",
    "\n",
    "HazaÃ«l-Massieux, D. (2016) W3C. ``_Javascript web apis - w3c_''.  [online](https://www.w3.org/standards/webdesign/script)\n",
    "\n",
    "Help:infobox, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/wiki/Help:Infobox)\n",
    "\n",
    "Json-ld, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=JSON-LD&oldid=841694454)\n",
    "\n",
    "McKinney, W. (2017). Python for data analysis, Data wrangling with pandas, numpy, and ipython (2nd edition).\n",
    "\n",
    "Microservices, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Microservices&oldid=848990555)\n",
    "\n",
    "Motto, 2018. Todd Motto, Github - toddmotto/public-apis: a collective list of public json apis for use in web development. Retrieved July 2018. [online](https://github.com/toddmotto/public-apis)\n",
    "\n",
    "Mozilla and individual contributors, 2018. ``_CSS basics_''.  [online](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/CSS_basics)\n",
    "\n",
    "Resource description framework, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Resource_Description_Framework&oldid=836549456)\n",
    "\n",
    "Schema.org community, (2018). ``_Schema.org_''.  [online](https://schema.org/)\n",
    "\n",
    "Semantic Web, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Semantic_Web&oldid=847416702)\n",
    "\n",
    "Software as a service, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Software_as_a_service&oldid=845520263)\n",
    "\n",
    "W3C Community, 2016. X. M. L. Core Working Group, Extensible markup language (xml). [online](https://www.w3.org/XML/Core/)\n",
    "\n",
    "W3C Community, 2018. W3C. Web Platform Working Group, ``_W3c html_'' [online](https://www.w3.org/html/)\n",
    "\n",
    "Web 2.0, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/wiki/Web_2.0)\n",
    "\n",
    "Web3D Consortium, 2018. Web3d consortium | open standards for real-time 3d communication.  [online](http://www.web3d.org/)\n",
    "\n",
    "Web ontology language, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Web_Ontology_Language&oldid=836282096)\n",
    "\n",
    "WHATWG (Apple, Google, Mozilla, Microsoft), 2018. Html living standard, 2018.  [online](http://www.whatwg.org/specs/web-apps/current-work/multipage/)\n",
    "\n",
    "Wiki, nd. Wikipedia, the free encyclopedia, retrieved Aug 20, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Wiki&oldid=848864746)\n",
    "\n",
    "World wide web consortium (w3c),  2018.  [online](https://www.w3.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "496px",
    "width": "386px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
