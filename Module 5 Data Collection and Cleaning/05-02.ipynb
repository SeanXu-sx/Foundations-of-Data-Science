{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Data Collection & Cleaning Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "This module consists of 3 parts.\n",
    "\n",
    "* Part 1 - Data Sources\n",
    "* Part 2 - Web Scraping\n",
    "* Part 3 - Data Preparation\n",
    "\n",
    "Each part is provided in a separate file. It is recommended that you follow the order of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Module-5:-Data-Collection-&amp;-Cleaning\" data-toc-modified-id=\"Module-5:-Data-Collection-&amp;-Cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Module 5: Data Collection &amp; Cleaning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Learning Outcomes</a></span></li><li><span><a href=\"#Readings-and-Resources\" data-toc-modified-id=\"Readings-and-Resources-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Readings and Resources</a></span></li><li><span><a href=\"#Web-Scraping\" data-toc-modified-id=\"Web-Scraping-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Web Scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#HTML-&amp;-XML\" data-toc-modified-id=\"HTML-&amp;-XML-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>HTML &amp; XML</a></span></li><li><span><a href=\"#JSON\" data-toc-modified-id=\"JSON-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>JSON</a></span></li></ul></li><li><span><a href=\"#Exercise-1\" data-toc-modified-id=\"Exercise-1-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Exercise 1</a></span></li><li><span><a href=\"#Scraping-the-Easy-Way\" data-toc-modified-id=\"Scraping-the-Easy-Way-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Scraping the Easy Way</a></span><ul class=\"toc-item\"><li><span><a href=\"#Python-Libraries-for-Web-Scraping\" data-toc-modified-id=\"Python-Libraries-for-Web-Scraping-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Python Libraries for Web Scraping</a></span></li><li><span><a href=\"#Links\" data-toc-modified-id=\"Links-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>Links</a></span></li></ul></li></ul></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Now that we have an idea of what the data looks like in terms of form, we can begin actually scraping for data. Here we cover JSON, and XML as these formats cover the majority of web sources. As a special case we also include HTML.\n",
    "Previously, we showed how to download a CSV file from a website and loaded into a usable `DataFrame` object. In this section we focus on doing the same so that you'll be able to fully utilize web resources as data. \n",
    "\n",
    "### HTML & XML\n",
    "Python has many libraries for reading and writing data in the HTML and XML formats. `lxml` (Behnel, 2018) is a python library that has consistently strong performance in parsing very large files.\n",
    "Lets begin using `lxml` by scraping the HTML. \n",
    "\n",
    "**NOTE**: In the following example, we can't use `pandas` to scrape. `pd.read_html(trickySite)` will throw an error.  This is because the `<table>` element isn't used by the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code and importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re as re\n",
    "np.random.seed(12345)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('figure', figsize=(10, 6))\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.options.display.max_rows = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "trickySite = 'https://xmarquez.github.io/democracyData/reference/pacl.html'\n",
    "# To get started, find the URL you want to extract data from, open it with *requests*.\n",
    "r = requests.get(trickySite, verify=False, )\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what? We know we have to extract the data, but wasn't this the entire point of using `pandas`? So that we wouldn't have to do this part?\n",
    "\n",
    "Unfortunately, this part can get very messy, especially for a web-scraping novice. Instead, we will make a simplifying assumption. ***HTML is well formed XML***. If this is true, then we should be able to manipulate and select items as if they are XML. The easiest way to do this is with * XPath * and * XSLT *.\n",
    "\n",
    "##### XPath\n",
    "**XML Path Language (XPath)** is a query language for selecting nodes from XML or to compute values from XML. XPath is simply a way to select groups of elements, attributes, text, etc., based on XML tree structure.  \n",
    "\n",
    "For example, the XPath value `/html/body/a` would grab all `a` elements directly in the `body` element in a root `html` element. However, if an `a` element is embedded inside another element in `body`, then it will not be retrieved. XPath can be thought of as a query language using a syntax similar to file directories. File directories can be absolute or relative. In our example, the path was absolute. If we wanted all `a` elements at any depth inside `body`, then we would use `/html/body//a` as our XPath.\n",
    "\n",
    "Generally, XPath is used with XML. Since XML defines other formats, XPath is a very useful technology to know when dealing with any markup language. It serves the exact same purpose as Regular Expressions, except for matching tree structures and branches in XML.\n",
    "\n",
    "#####  XSLT\n",
    "\n",
    "So far the use cases we've discussed are as follows. \n",
    "\n",
    "* XML is data\n",
    "* XPath is a way to query XML\n",
    "\n",
    "**eXtensible Stylesheet Language Transformations (XSLT)** would be best described as a way to transform, merge, join, and generally perform operations with and on XML. The use-case of XSLT is to provide a way to rewrite XML data into other formats. XSLT is a subset of XSL, a stylesheet language. Yet, in order for XSLT to remain general, nothing technology specific can be used. This results in a language specifically designed to be good with expressing transformations concisely for XML. \n",
    "\n",
    "**NOTE**: XSLT is itself expressed as an XML data format.\n",
    "\n",
    "For those paying close attention, notice that XPath is the equivalent to a CSS selector. Both need to find groups of elements and then apply their respective snippets of stylesheet code onto them.\n",
    "\n",
    "Consider the following benefits of these technologies so far.\n",
    "\n",
    "* XML data is guaranteed to be cleanly parsed. \n",
    "* XPath insures subsets of documents can be retrieved without problems. \n",
    "* XSLT insures elements can be rewritten from XML input to any output. \n",
    "\n",
    "*** Thus XML, XPath, and XSLT becomes a powerful generic data manipulation pipeline. *** But more importantly, it allows us to transform the elements we wish to extract into an HTML table, while ignoring all other elements. We are now able to do all of this in very few lines of code \\cite{Behnel2018}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.671Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lxml\n",
    "import lxml.etree\n",
    "\"\"\"\n",
    "1. find the root\n",
    "2. match on all `dl` tags and begin producing a `table` of them\n",
    "3. match on all `dd` and `dt` tags and \n",
    "  * place them into their own rows (<tr> tags)\n",
    "    * inside their own cells (<td> tags)\n",
    "\"\"\"\n",
    "xslt_root = lxml.etree.XML('''\\\n",
    " <xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\">\n",
    "     <xsl:template match=\"/\">\n",
    "        <html><body>\n",
    "             <xsl:for-each select=\".//dl\">\n",
    "                 <table>\n",
    "                     <tr>\n",
    "                         <xsl:for-each select=\".//dd\">\n",
    "                         <td>\n",
    "                             <xsl:value-of select=\".\" />\n",
    "                         </td>\n",
    "                         </xsl:for-each>\n",
    "                     </tr>\n",
    "                     <tr>\n",
    "                         <xsl:for-each select=\".//dt\">\n",
    "                             <td>\n",
    "                                 <xsl:value-of select=\".\" />\n",
    "                             </td>\n",
    "                         </xsl:for-each>\n",
    "                     </tr>\n",
    "                 </table>\n",
    "             </xsl:for-each>\n",
    "         </body></html>\n",
    "     </xsl:template>\n",
    " </xsl:stylesheet>''')\n",
    "\n",
    "# compile it from the XSLT\n",
    "transform = lxml.etree.XSLT(xslt_root)\n",
    "\n",
    "# apply the XSLT transformation to an HTML compiled representation of requested page\n",
    "tree_to_scrape = transform(lxml.etree.HTML(r.content))\n",
    "\n",
    "## Return string of output. Uncomment to view output\n",
    "# str(tree_to_scrape)\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:03.742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>order</td>\n",
       "      <td>Sequential numbering of rows (1 through 9159)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pacl_country</td>\n",
       "      <td>String country identifier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>year</td>\n",
       "      <td>Calendar year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>agedem</td>\n",
       "      <td>Age in years of the current regime as classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>agereg</td>\n",
       "      <td>Age in years of the current regime as classifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>stra</td>\n",
       "      <td>Sum of past transitions to authoritarianism in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1                                                  0\n",
       "0          order      Sequential numbering of rows (1 through 9159)\n",
       "1   pacl_country                         String country identifier.\n",
       "2           year                                      Calendar year\n",
       "..           ...                                                ...\n",
       "68        agedem  Age in years of the current regime as classifi...\n",
       "69        agereg  Age in years of the current regime as classifi...\n",
       "70          stra  Sum of past transitions to authoritarianism in...\n",
       "\n",
       "[71 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. convert to string\n",
    "2. read with pandas\n",
    "3. grab the first/zeroth table\n",
    "4. transpose it\n",
    "5. switch indexing order for columns (optional)\n",
    "\"\"\" \n",
    "firstTable = pd.read_html(str(tree_to_scrape))[0].T[[1,0]]\n",
    "\n",
    "# return pandas `DataFrame` \n",
    "firstTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "\n",
    "**JavaScript Object Notation (JSON)** has become one of the standard formats for sending data by HTTP request between web browsers and other applications. It is a much more flexible data format than a tabular text form like CSV, and much smaller in size than XML files. JSON is very nearly valid Python code. \n",
    "\n",
    "To convert a JSON string to Python form, use `json.loads`. `json.dumps` on the other hand converts a Python object back to JSON. Because the JSON structure is unpredictable, how you convert a JSON object or list of objects to a DataFrame or some other data structure for analysis will be up to you. Conveniently, we are able to pass a list of JSON objects to the `DataFrame` constructor and select subsets of fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:04.386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>2018-08-14T20:17:08Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <td>en-US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>results</th>\n",
       "      <td>{'channel': {'units': {'distance': 'mi', 'pres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     query\n",
       "count                                                    1\n",
       "created                               2018-08-14T20:17:08Z\n",
       "lang                                                 en-US\n",
       "results  {'channel': {'units': {'distance': 'mi', 'pres..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Endpoint for retrieving Yahoo Weather for Toronto, ON. \n",
    "apiEndpoint =  'https://query.yahooapis.com/v1/public/yql?q=select%20*%20from%20weather.forecast%20where%20woeid%20in%20(select%20woeid%20from%20geo.places(1)%20where%20text%3D%22toronto%2C%20on%22)&format=json&env=store%3A%2F%2Fdatatables.org%2Falltableswithkeys'\n",
    "\n",
    "\n",
    "response_json = pd.read_json(apiEndpoint, lines=False)\n",
    "response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a pandas `DataFrame` from the results and not the response? `results` actually contains a proper python object. So, it can be manually manipulated or we can reapply pandas to `results`.\n",
    "\n",
    "And, this can be repeated. The best way to repeat would be to apply `map()` and then combine horizontally or vertically depending on the situation. Moreover, dummy variables per expansion would allow checking a field's presence. Additionally, it would be preferable to wrap these in a python function to compartmentalize the code used for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-06-20T16:52:04.789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>astronomy</th>\n",
       "      <td>{'sunrise': '6:21 am', 'sunset': '8:23 pm'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atmosphere</th>\n",
       "      <td>{'humidity': '70', 'pressure': '999.0', 'risin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <td>Yahoo! Weather for Toronto, ON, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ttl</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>{'distance': 'mi', 'pressure': 'in', 'speed': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wind</th>\n",
       "      <td>{'chill': '77', 'direction': '185', 'speed': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       channel\n",
       "astronomy          {'sunrise': '6:21 am', 'sunset': '8:23 pm'}\n",
       "atmosphere   {'humidity': '70', 'pressure': '999.0', 'risin...\n",
       "description                 Yahoo! Weather for Toronto, ON, CA\n",
       "...                                                        ...\n",
       "ttl                                                         60\n",
       "units        {'distance': 'mi', 'pressure': 'in', 'speed': ...\n",
       "wind         {'chill': '77', 'direction': '185', 'speed': '...\n",
       "\n",
       "[13 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(response_json[\"query\"][\"results\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People are always being elected and unelected. It can be hard to keep up. Why not write a web scraper?\n",
    "Extract the *Members of Parliament* in the [Canadian House of Commons](https://www.ourcommons.ca/) (House of Commons, 2018) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code Here\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Honorific Title</th>\n",
       "      <th>First Name</th>\n",
       "      <th>Last Name</th>\n",
       "      <th>Constituency</th>\n",
       "      <th>Province / Territory</th>\n",
       "      <th>Political Affiliation</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ziad</td>\n",
       "      <td>Aboultaif</td>\n",
       "      <td>Edmonton Manning</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dan</td>\n",
       "      <td>Albas</td>\n",
       "      <td>Central OkanaganSimilkameenNicola</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Harold</td>\n",
       "      <td>Albrecht</td>\n",
       "      <td>KitchenerConestoga</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>NaN</td>\n",
       "      <td>David</td>\n",
       "      <td>Yurdiga</td>\n",
       "      <td>Fort McMurrayCold Lake</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Salma</td>\n",
       "      <td>Zahid</td>\n",
       "      <td>Scarborough Centre</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Zimmer</td>\n",
       "      <td>Prince GeorgePeace RiverNorthern Rockies</td>\n",
       "      <td>British Columbia</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>10/19/2015 12:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Honorific Title First Name  Last Name  \\\n",
       "0               NaN       Ziad  Aboultaif   \n",
       "1               NaN        Dan      Albas   \n",
       "2               NaN     Harold   Albrecht   \n",
       "..              ...        ...        ...   \n",
       "332             NaN      David    Yurdiga   \n",
       "333             NaN      Salma      Zahid   \n",
       "334             NaN        Bob     Zimmer   \n",
       "\n",
       "                                   Constituency Province / Territory  \\\n",
       "0                              Edmonton Manning              Alberta   \n",
       "1           Central OkanaganSimilkameenNicola     British Columbia   \n",
       "2                           KitchenerConestoga              Ontario   \n",
       "..                                          ...                  ...   \n",
       "332                     Fort McMurrayCold Lake              Alberta   \n",
       "333                          Scarborough Centre              Ontario   \n",
       "334  Prince GeorgePeace RiverNorthern Rockies     British Columbia   \n",
       "\n",
       "    Political Affiliation              Start Date  End Date  Unnamed: 8  \n",
       "0            Conservative  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "1            Conservative  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "2            Conservative  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "..                    ...                     ...       ...         ...  \n",
       "332          Conservative  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "333               Liberal  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "334          Conservative  10/19/2015 12:00:00 AM       NaN         NaN  \n",
       "\n",
       "[335 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SOLUTION\n",
    "# If they inspect the site and don't jump the gun, they'll discover the the Gov. Open Data intiatives are alive and well.\n",
    "r = requests.get('https://www.ourcommons.ca/Parliamentarians/en/members/export?output=CSV&listSeperator=,')\n",
    "pd.read_csv(filepath_or_buffer=io.StringIO(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Easy Way\n",
    "\n",
    "There is of course nothing wrong with a more interactive or manual approach for webscraping using more interactive tools. We only focus on automating the process for repetition. But it should be noted that websites are not static and will often change. For example, people profiles for Members of Parliament change on a regular basis. \n",
    "\n",
    "\n",
    "Below are some alternatives for scraping\n",
    "\n",
    "* [Google Chrome Web Scraper extension](http://webscraper.io/)\n",
    "* [import.io](http://import.io)\n",
    "* [Spooky Stuff](https://github.com/tribbloid/spookystuff)\n",
    "\n",
    "### Python Libraries for Web Scraping\n",
    "There are many libraries that can be used for scraping web resources. We chose to use the `requests` library largely due to its ability to fully interact with Web APIs and its ability to reduce most use-cases to one-line of code in comparison to other libraries. Similarly, we make use of `lxml` due to its support for XML, XPath, and XSLT for easier transformation into a format immediately useable by `pandas`.Below are the libraries in Python that one would find relevant for the web scraping domain.\n",
    "\n",
    "* [urllib2](https://docs.python.org/3.6/library/urllib.request.html)\n",
    "* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "  * **NOTE**: `lxml` is used by the library.\n",
    "* [scrapy](https://docs.scrapy.org/en/latest/)\n",
    "  * **NOTE**: library is for crawling entire websites, and not  requesting only one specific Web Resource.\n",
    "* [requests](http://docs.python-requests.org/en/master/)\n",
    "* [RoboBrowser](https://robobrowser.readthedocs.io/en/latest/readme.html)\n",
    "  * For scraping and filling out forms automatically. Does not render dynamic pages.\n",
    "* [lxml](http://lxml.de/)\n",
    "\n",
    "One topic we don't cover are headless web browser, which simulate an actual browser, enabling scraping from the *Document Object Model*, allowing for web scraping of dynamic web resources.  The following are libraries and tools specifically for addressing this use-case.\n",
    "\n",
    "* [splash](https://splash.readthedocs.io/en/stable/)\n",
    "* [selenium](https://www.seleniumhq.org/)\n",
    "* [Splinter](https://splinter.readthedocs.io/en/latest/)\n",
    "* [phantompy](https://phantompy.readthedocs.io/en/latest/)\n",
    "  * [phantomjs](http://phantomjs.org/)\n",
    "  \n",
    "## End of Part 2\n",
    "\n",
    "This notebook makes up one part of this module. Now that you have completed this part, please proceed to the next notebook in this module.\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "* http://blog.miguelgrinberg.com/post/easy-web-scraping-with-python\n",
    "  * About scraping using other python libraries, as well as crawling entire websites.\n",
    "* http://scrapy.org/\n",
    "  * About writing scrapers as configeration files via scrapy.\n",
    "* https://docs.python.org/2/library/urllib2.html\n",
    "  * Documentation for urlib2 library\n",
    "* http://docs.python-requests.org/en/latest/\n",
    "* The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)\n",
    "  * https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/\n",
    "* http://import.io\n",
    "  * A web-based platform for extracting data from websites without writing any code.\n",
    "* http://www.crummy.com/software/BeautifulSoup/\n",
    "  * Popular alternative to lxml for web/screen scraping\n",
    "* http://pbpython.com/web-scraping-mn-budget.html\n",
    "  * Tutorial using BeautifulSoup with requests library, pandas, numpy and mathplotlib\n",
    "* Python Regular Expressions Cheat Sheet\n",
    "  * https://pycon2016.regex.training/cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Behnel, S. et al, (2018). Xpath and xslt with lxml [online](http://lxml.de/xpathxslt.html)\n",
    "\n",
    "House of Commons, (2018). Members of Parliament. Retrieved Aug 21, 2018 from http://www.ourcommons.ca/Parliamentarians/en/members\n",
    "\n",
    "McKinney, W. (2017). Python for data analysis: Data wrangling with Pandas, NumPy, and IPython (2nd Ed.). O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "496px",
    "width": "386px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "374px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
