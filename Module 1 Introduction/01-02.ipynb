{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Module 1: Introduction to Data Science - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "This module consists of two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "* Part 1 - Setup and Installation Instructions\n",
    "* Part 2 - Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "Each part is provided in a separate notebook file. It is recommended that you follow the order of the notebooks. This module part provides a general overview of what constitutes Data Science, and the trends affecting it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Outcomes\n",
    "\n",
    "In this module, you will learn the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The history of Data Science and its transition from traditional analytics\n",
    "* The difference between data analyst and scientist roles\n",
    "* The major technology shifts driving modern Data Science\n",
    "* Current Data Science applications and use cases in industry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings and Resources\n",
    "\n",
    "There are no recommended readings and resources for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Module-1:-Introduction-to-Data-Science---Part-2\" data-toc-modified-id=\"Module-1:-Introduction-to-Data-Science---Part-2-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Module 1: Introduction to Data Science - Part 2</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Learning Outcomes</a></span></li><li><span><a href=\"#Readings-and-Resources\" data-toc-modified-id=\"Readings-and-Resources-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Readings and Resources</a></span></li><li><span><a href=\"#Introduction-to-Data-Science\" data-toc-modified-id=\"Introduction-to-Data-Science-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Introduction to Data Science</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-Brief-History-of-Data-Science\" data-toc-modified-id=\"A-Brief-History-of-Data-Science-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>A Brief History of Data Science</a></span></li><li><span><a href=\"#The-Evolution-of-Analytics\" data-toc-modified-id=\"The-Evolution-of-Analytics-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>The Evolution of Analytics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Analytics-1.0:-Traditional-Analytics-(to-mid-2000s)\" data-toc-modified-id=\"Analytics-1.0:-Traditional-Analytics-(to-mid-2000s)-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Analytics 1.0: Traditional Analytics (to mid-2000s)</a></span></li><li><span><a href=\"#Analytics-2.0:-Big-Data-(mid-2000s)\" data-toc-modified-id=\"Analytics-2.0:-Big-Data-(mid-2000s)-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Analytics 2.0: Big Data (mid-2000s)</a></span></li><li><span><a href=\"#Analytics-3.0:-Prescriptive-Analytics-(emerging-now)\" data-toc-modified-id=\"Analytics-3.0:-Prescriptive-Analytics-(emerging-now)-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Analytics 3.0: Prescriptive Analytics (emerging now)</a></span></li></ul></li><li><span><a href=\"#From-Data-Analysts-to-Data-Scientists\" data-toc-modified-id=\"From-Data-Analysts-to-Data-Scientists-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>From Data Analysts to Data Scientists</a></span><ul class=\"toc-item\"><li><span><a href=\"#Traditional-Analysts\" data-toc-modified-id=\"Traditional-Analysts-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Traditional Analysts</a></span></li><li><span><a href=\"#Data-Scientists\" data-toc-modified-id=\"Data-Scientists-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Data Scientists</a></span></li></ul></li><li><span><a href=\"#Data-Science-is-Multidisciplinary\" data-toc-modified-id=\"Data-Science-is-Multidisciplinary-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Data Science is Multidisciplinary</a></span></li><li><span><a href=\"#Big-Data\" data-toc-modified-id=\"Big-Data-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Big Data</a></span></li><li><span><a href=\"#Predictive-Modeling\" data-toc-modified-id=\"Predictive-Modeling-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Predictive Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Predictive-Modeling-Process\" data-toc-modified-id=\"The-Predictive-Modeling-Process-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>The Predictive Modeling Process</a></span></li></ul></li><li><span><a href=\"#Data-Mining\" data-toc-modified-id=\"Data-Mining-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Data Mining</a></span></li><li><span><a href=\"#The-Analytics-Pipeline\" data-toc-modified-id=\"The-Analytics-Pipeline-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>The Analytics Pipeline</a></span></li><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;</span>Machine Learning</a></span></li><li><span><a href=\"#Artificial-Intelligence\" data-toc-modified-id=\"Artificial-Intelligence-5.10\"><span class=\"toc-item-num\">5.10&nbsp;&nbsp;</span>Artificial Intelligence</a></span></li><li><span><a href=\"#Applications-of-Predictive-Modeling\" data-toc-modified-id=\"Applications-of-Predictive-Modeling-5.11\"><span class=\"toc-item-num\">5.11&nbsp;&nbsp;</span>Applications of Predictive Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Applications-in-Retail\" data-toc-modified-id=\"Applications-in-Retail-5.11.1\"><span class=\"toc-item-num\">5.11.1&nbsp;&nbsp;</span>Applications in Retail</a></span><ul class=\"toc-item\"><li><span><a href=\"#Segmentation\" data-toc-modified-id=\"Segmentation-5.11.1.1\"><span class=\"toc-item-num\">5.11.1.1&nbsp;&nbsp;</span>Segmentation</a></span></li><li><span><a href=\"#Recommenders\" data-toc-modified-id=\"Recommenders-5.11.1.2\"><span class=\"toc-item-num\">5.11.1.2&nbsp;&nbsp;</span>Recommenders</a></span></li><li><span><a href=\"#Market-Basket-Analysis\" data-toc-modified-id=\"Market-Basket-Analysis-5.11.1.3\"><span class=\"toc-item-num\">5.11.1.3&nbsp;&nbsp;</span>Market Basket Analysis</a></span></li><li><span><a href=\"#Churn-Prevention\" data-toc-modified-id=\"Churn-Prevention-5.11.1.4\"><span class=\"toc-item-num\">5.11.1.4&nbsp;&nbsp;</span>Churn Prevention</a></span></li></ul></li><li><span><a href=\"#Applications-in-Financial-Services\" data-toc-modified-id=\"Applications-in-Financial-Services-5.11.2\"><span class=\"toc-item-num\">5.11.2&nbsp;&nbsp;</span>Applications in Financial Services</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fraud-Detection\" data-toc-modified-id=\"Fraud-Detection-5.11.2.1\"><span class=\"toc-item-num\">5.11.2.1&nbsp;&nbsp;</span>Fraud Detection</a></span></li><li><span><a href=\"#Financial-Markets\" data-toc-modified-id=\"Financial-Markets-5.11.2.2\"><span class=\"toc-item-num\">5.11.2.2&nbsp;&nbsp;</span>Financial Markets</a></span></li></ul></li><li><span><a href=\"#Applications-in-Healthcare\" data-toc-modified-id=\"Applications-in-Healthcare-5.11.3\"><span class=\"toc-item-num\">5.11.3&nbsp;&nbsp;</span>Applications in Healthcare</a></span><ul class=\"toc-item\"><li><span><a href=\"#Diagnosis\" data-toc-modified-id=\"Diagnosis-5.11.3.1\"><span class=\"toc-item-num\">5.11.3.1&nbsp;&nbsp;</span>Diagnosis</a></span></li><li><span><a href=\"#Drug-Discovery\" data-toc-modified-id=\"Drug-Discovery-5.11.3.2\"><span class=\"toc-item-num\">5.11.3.2&nbsp;&nbsp;</span>Drug Discovery</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#In-coming-weeks\" data-toc-modified-id=\"In-coming-weeks-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>In coming weeks</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Data Science” is a fairly new term, for a new profession that applies the Scientific Method to analysis of data, and in particular, Big Data. Collecting, storing, and making sense of Big Data (another fairly new term) is quickly becoming part of every business and everyone’s life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief History of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are a few of the key trends and milestones that led us to modern Data Science:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Year(s)*** | ***Trends*** |\n",
    "| ---: | :--- |\n",
    "| **1960s-1970s** | Rapid advances in statistics and computer science |\n",
    "| **Late 1990s** | Google invented a new search engine combining math, statistics, data engineering and computation |\n",
    "| **2000s** | Hard drives and computational power (particularly floating point computation using graphics cards) continued to become less expensive |\n",
    "| **2001** | The term *Data Science* was coined|\n",
    "| **2002** | CODATA Data Science journal launched |\n",
    "| **2003** | Columbia University began publishing *The Journal of Data Science*. Google File System paper spawns *Hadoop* |\n",
    "| **2006** | Moore's Law came to an end so increasing computer power started to require switching to parallel processing on multiple CPUs |\n",
    "| **2010s** | Rapid development of Deep Learning (Deep Learning, nd) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Evolution of Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 1.0: Traditional Analytics (to mid-2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional Analytics was primarily reactive:\n",
    "- Primarily descriptive and focus on reporting\n",
    "- Internally sources, relatively small, structured data\n",
    "- \"Backroom\" teams of analysts\n",
    "- Internal Systems of Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 2.0: Big Data (mid-2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern Analytics involves sophisticated modelling and in some cases complex data streaming and processing infrastructure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complex, large, unstructured data sources\n",
    "- Stored and processed rapidly, with new analytical and computational technologies\n",
    "- \"Data Scientists\" emerge\n",
    "- Online firms create data-based products and services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics 3.0: Prescriptive Analytics (emerging now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are entering a new era in which predictive models will guide business leaders in real-time decision-making:\n",
    "- An environment which combines Analytics 1.0 and 2.0 that yields insights with speed and impact\n",
    "- Analytics is integral to running a business and becomes part of strategy and operations\n",
    "- Predictive and Prescriptive Models--models that recommend specific actions\n",
    "- Artificial Intelligence techniques such as Deep Learning and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Data Analysts to Data Scientists\n",
    "The role of the data specialist has been evolving as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Analysts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional data analysts can be found in many corporate departments, and in particular Finance, Marketing and IT (Business Intelligence).  They've tended to be reactive, responding to specific requests for information or developing and overseeing regular reporting.  They have tended to use proprietary (i.e. not open source) tools such as *Excel*, *SAS*, *SPSS*, *Cognos*, etc. and often have good *SQL* database query skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern data scientists tend to use and often have a preference for open source tools such as *Python* and *R* in addition to the traditional toolsets.  For large volumes of data they typically use a Big Data analytics system such as *Spark*. Once you learn skills and tools in one environment you can easily transition to the other. The underlying skills are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science is Multidisciplinary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<figure>\n",
    "    <img src=\"dataScienceOverlap.png\" alt=\"Data Science is oriented at the intersection of a large number of traditional disciplines. Currently, we recognize that Data Scientists encompass expertise from the fields of mathematics, statistics, machine learning and data mining, business, software engineering, data engineering, programming, visualization, storytelling and subject area expertise.\" style=\"width: 62%;\">\n",
    "    <figcaption><em>Data Science is oriented at the intersection of a large number of traditional disciplines. Currently, we recognize that Data Scientists encompass expertise from the fields of mathematics, statistics, machine learning and data mining, business, software engineering, data engineering, programming, visualization, storytelling and subject area expertise. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science draws on a wide variety of previously existing disciplines, skills and bodies of knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Discipline/Skill/Knowledge*** | ***Reasoning*** |\n",
    "| ---: | :--- |\n",
    "| **Mathematics** | Data Science uses mathematical techniques to model phenomena in the world or in business so as to gain useful insights or draw conclusions with (where possible) a measure of how much confidence one should put in the conclusion |\n",
    "| **Statistics** | It's the use of statistical procedures that enables these soft conclusions to be made or insights to be taken from large volumes of data by looking for correlations (relationships) between variables |\n",
    "| **Machine Learning and Data Mining** | Techniques for automated detection of these correlations are the \"core technology\" of data science |\n",
    "| **Business** | In a business setting, all of the usual business-related skills are essential. *(i.e., the ability to make tradeoffs between investment in data science staff and tools vs. likely payoffs, working with other departments to identify potential data science projects and quantifying their value, negotiating for availability of resources, etc.)* |\n",
    "| **Software Engineering** | Often predictive models developed by a corporate data science team will be deployed into the data processing infrastructure of the company, which means they must be designed considering tradeoffs such as their throughput and latency in addition to their predictive power |\n",
    "| **Data Engineering** | When dealing with \"Big Data\" (defined below), new scalable data streaming and distributed database technologies come into play, and with them new issues and strategies for dealing with them |\n",
    "| **Programming** | Although the machine learning part of data science is mostly done using tools that have been developed by specialists, there is a lot of work to get a dataset prepared for analysis, and this often requires writing custom scripts to clean up the data |\n",
    "| **Visualization** | Often relationships in data are easier to see in pictures than numbers and can be much more convincing |\n",
    "| **Story Telling** | At the end of an analysis a data scientist must be able to tell a story about how their conclusions fit together into a compelling story, especially if some kind of action must be taken by others as a result |\n",
    "| **Subject Area Expertise** | Data science is a tool in service of business, social goals or other branches of science; all of your existing skills in the areas you already know will help you to identify worthwhile projects to turn the data science lens to |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientists are rarely deep experts in more than one or two of these areas but always have competence in basic statistics and a working knowledge of most other areas.  Math and statistics are core skills but a pure degree in statistics isn't usually necessary.  Most data scientists come from a *STEM (Science, Technology, Engineering and Math)* background but artists with expertise in visualization and storytelling are equally as important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "*Big Data* is a relatively new term, however collecting, storing and analysing data is centuries old. The concept gained momentum in the early 2000s when industry analyst *Doug Laney* articulated the now-mainstream definition of Big Data as the three V's: *Velocity, Variety and Volume* (Downey, 2015). Data becomes \"Big\" when the rate at which it arrives, the variety of data formats you need to deal with and/or the quantities to be processed grow to a level at which a single-server processing solution will no longer be adequate.  Relational databases can't be distributed across multiple servers (not beyond a very few), and synchronization between them quickly becomes impossible when dealing with large or globally distributed data sources.  Big Data inevitably involves new technologies designed for distributed, eventual data consistency and so a new branch of *Data Science* known as *Data Engineering* has emerged.\n",
    "\n",
    "Others have extended Laney's V's in various ways, for example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"bigDataVs.png\" alt=\"This image shows the extensions of Laney's V's as a diagram.\" style=\"width: 38%;\">\n",
    "    <figcaption><em>This image shows the extensions of Laney's V's as a diagram. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally you may see references to another V, \"Veracity\" which means truthfulness and can be considered to encompass correctness and accuracy.\n",
    "\n",
    "In this image we see the original three V's enhanced with another three key characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Additional Characteristics*** | ***Reasoning*** |\n",
    "| ---: | :--- |\n",
    "| **Variability** | In the online world systems can go from very quiet to massive demand in a matter of seconds and should be able scale up and down quickly in response |\n",
    "| **Complexity** | The kinds of data available and the variety of ways it can be encoded with descriptive tags has exploded |\n",
    "| **Value** | Does acquiring this data represent good value for the costs involved? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling\n",
    "\n",
    "Predictive Modelling is a process of creating a model that, given a set of inputs, will produce a plausible output similar to some business, economic, human, natural or other process it is meant to mimic.\n",
    "Predictive models are usually statistical in nature. We don't usually know the precise mapping of inputs to outputs or even what all of the input variables are much less their current values.\n",
    "So we can't define a precise mathematical function but there may be significant value in having a good approximation.\n",
    "\n",
    "The development of the discipline of Machine Learning gave us tools for discovering such approximations by analyzing the patterns in large datasets.\n",
    "With a large enough data set of examples of real inputs and outputs these tools can learn a good input-to-output mapping.\n",
    "\n",
    "*i.e.,*\n",
    "* *which behaviours (input) are correlated with subsequent fraud (output)* \n",
    "* *which photos (input) do and do not contain a picture of a goat (output)*\n",
    "\n",
    "The rise of the Internet has enabled capturing huge data sets that make this kind of analysis feasible and has led to the recent advances in areas such as face recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Predictive Modeling Process\n",
    "The process of developing a predictive model involves several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"dataScienceProcess.png\" alt=\"This image shows the stages of creating a model from start to finish.\" style=\"width: 100%;\"> (Course Authors, 2018)\n",
    "    <figcaption><em>This image shows the stages of creating a model from start to finish.</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Steps*** | ***Description*** |\n",
    "| ---: | :--- |\n",
    "| **Define Problem** | Especially in a business context, the best results in terms of value-for-money come from tackling a *well-defined* problem. The solution to the problem may not be entirely evident at first and some exploration in terms of data collection and analysis may be required, but it is best to have a specific objective in mind. |\n",
    "| **Prepare Data** | This involves finding the data you need, which may be from several sources, confirming its suitability and transforming it into a format amenable to subsequent analysis. |\n",
    "| **Create Model** | This step involves an initial analysis of the dataset to find useful patterns and selecting one or more modelling approaches.  Data scientists have an array of tools and methods to choose from at their disposal.  If Machine Learning techniques are used this stage will also involve *training* the model on a dataset of previously-observed results for given inputs to the process being modeled. |\n",
    "| **Test Model** | This phase involves confirming that the model is operating correctly as expected. |\n",
    "| **Validate Model** | In this step, the model is presented with test cases of inputs and outputs similar to those used for training the model to confirm that it makes predictions that are sufficiently similar to be useful.  As we will see later, we almost always want the model to *generalize* and not rote-learn an input-to-output mapping.  Validating with data that *was not used to train the model* will help us avoid models that have not learned to generalize. |\n",
    "| **Evaluate Model** | Data scientists will often develop several alternative models and compare their performance so as to choose from among the best.  Counterintuitively, sometimes the best model is a weighted average of several models (an *ensemble* of models) that are not the best performers individually. |\n",
    "| **Deploy Model** | Increasingly, in a business context this means making the model part of an on-going process where continually-arriving data updates the models predictions and those predictions are used to create additional business value daily. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Mining\n",
    "Sometimes the focus is less on developing a predictive model and more on discovering relationships within a dataset that\n",
    "may represent new and valuable insights.  The term Data Mining is used for these situations.  For example, you may have a large\n",
    "dataset of network events captured from a credit card processing system and might be interested in patterns that represent\n",
    "attempts by hackers to break in.  Another example would be to identify behaviors of smartphone customers that are telltale\n",
    "signs that they are considering leaving for a competitor so that the company can proactively intervene and attempt to retain them.\n",
    "These insights can potentially then be used to formulate a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Analytics Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<figure>\n",
    "    <img src=\"dataCapture.png\" alt=\"This image shows the major stages of producing an analysis. (Course Authors, 2018)\" style=\"width: 62%;\">\n",
    "    <figcaption><em>This image shows the major stages of producing an analysis. (Course Authors, 2018)</em></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Many data-intensive companies have now deployed an Analytics Pipeline similar to the one shown in the diagram.  The details vary from organization to organization, but generally follow this broad pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ***Pipeline Stage*** | ***Description*** |\n",
    "| ---: | :--- |\n",
    "| **Data Sources**  | Data arrives from a variety of systems and providers.  Increasingly, this includes the so-called **Internet-of-Things (IoT)** i.e., devices attached to the Internet (*as opposed to people directly*).  Some examples: Cellphones generate a huge amount of streaming data about people's location, app usage, phone usage, etc.  In auto insurance and fleet management, real-time information about car and truck drivers' behavior is increasingly being monitored.  Sensors in modern tractors continually monitor the soil conditions for farmers and are sent to agricultural product manufacturers who use the information to recommend improvements to drainage, pesticide and fertilizer use.  A second source of data is a company's internal systems.  Banks use internally-sourced data aggregated from across their many lines of business to create a consolidated risk management view.  Retailers scour sales data from their *point-of-sale* terminals to spot trends and behaviors they can use to drive profitability.  For the large Internet companies (*Google, LinkedIn, etc.*) their users are a source of an ocean of real-time data about opinions, trends and behaviors at the individual level.  And, in our connected society, companies increasingly share data from outside their walls: with trading and supply chain partners, and by purchasing data from 3rd party data providers. |\n",
    "| **Data Capture and Processing** | The arriving data needs to be captured and saved for subsequent analysis.  Organizations are increasingly capturing data in its raw form in a database optimized for fast data input.  This is a departure from the past where data was usually carefully validated and often preprocessed before being stored, and was saved in a near-normalized form in a relational database.  In some applications today (especially Web or IoT-based) the arriving data rates are so high that data capture using relational technology is infeasibly slow.  Relatively new database technologies such as *Hadoop* were designed for this purpose: very fast writing of large volumes of data.  A database used this way, as a capture area for raw data is often called a *data landing area* or **data lake**.  In addition to there not being any time for preprocessing (*at least during periods of rapid data arrival*) the other reason for storing raw data is that preprocessing presupposes a particular use for the data and as a result throws away some of its content.  If the data is stored in its raw form, all of the information content is preserved in case at some point in the future additional forms of analysis might require it.  Once the data has landed, data scientists can use it to build models or conduct enquiries, often requiring transforming the data to improve its quality or make it more amenable to analysis.  Transformations and models that prove to be useful over the long term can be automated and deployed as on-going processes that produce regular outputs. |\n",
    "| **Analysis Results** | The outputs of this overall process include ad hoc and on-going analysis and predictions, standing reports and insights into the data that may lead to business process improvements or better margins. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "Through most of the twentieth century, the standard approach to building a predictive model started with identifying the features of the business, social or economic process of interest that were likely to have some predictive power.  In other words, we identified the independent variables (inputs) to the model in advance of building the model.  This approach is  known as **Feature Engineering** and is still important.  But we now have many tools to assist us in finding predictive variables empirically.\n",
    "\n",
    "Machine Learning is a term used to describe the automatic extraction of knowledge from data.  By knowledge, we mean a set of generalizations that can be made about the data.  These generalizations can be hard rules, statements that appear to always be true, at least within a given dataset, or soft rules that are approximately or probabilistically true.  Suppose, for example, you knew nothing about the rules of the road.  Analyzing a dataset of nothing but traffic patterns might allow you to infer that driving only one way on some streets is a hard rule (unless you have a dataset that's big enough to contain an example of someone doing it).  Speed limits would become apparent as a very soft rule (in fact, as a distribution around a speed no doubt a little above the posted limit).\n",
    "\n",
    "There are a variety of Machine Learning algorithms that have been developed for this purpose, each with their own strategy for extracting and representing knowledge: some as actual rules, some as tree structures, some as statistical distributions, often as combinations of these.  The process of extracting the knowledge from a dataset is called *training* or learning a model.\n",
    "\n",
    "Each algorithm has its own performance characteristics.  Some are fast to train but slow to compute a prediction when required whereas the opposite is true of others.  Some are more resilient to noise in the data than others.  Paradoxically, simple models often perform as well as highly sophisticated ones for many business applications.  We will meet many of these algorithms in courses in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Intelligence\n",
    "\n",
    "You may be wondering about the difference between Machine Learning and *Artificial Intelligence (AI)*.  Machine Learning is a particular set of knowledge and techniques within the much broader topic of AI.  \n",
    "\n",
    "* AI comprises the entire question of whether and how a machine can show behaviors that are sufficiently human-like to be indistinguishable from the real thing.  It includes general theories of knowledge representation and strategies for searching for solutions to complex problems in a large space of alternatives.  \n",
    "* Machine Learning is the subset of AI that is focused on extracting knowledge from observed data with little human guidance whereas AI more generally allows for incorporating generic or hand-crafted problem-solving strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Predictive Modeling\n",
    "Let's take a look at the myriad applications of Data Science and Predictive Modeling.  Here are a few industries we will focus on:\n",
    "- Retail\n",
    "- Financial Services\n",
    "- Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Retail\n",
    "#### Segmentation\n",
    "Customer segmentation is a type of predictive model.  For example, we would like to predict which segment a customer is likely to fall into, so that we can tailor our marketing strategy to meet their needs and increase their satisfaction.\n",
    "\n",
    "Why segment markets? Customers may differ in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What they want to buy\n",
    "* Amount willing to pay\n",
    "* Quantity they buy\n",
    "* Time, place, the frequency of purchase\n",
    "* Personal taste (likes and dislikes) in \n",
    "    * media\n",
    "    * telephone plan\n",
    "    * newspapers\n",
    "    * magazines\n",
    "    * movies\n",
    "    * social media\n",
    "    * etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a retailer knows that you belong to a segment that shops often, and on average spends \\$100 per transaction, their strategy may be to try to \"stretch you\" to spend \\$125 on your next purchase in the hope that it will become a new pattern.  They could do this by offering you a coupon, or a certain promotion (spend \\$150 and get \\$25 back).\n",
    "\n",
    "Some customers may even represent negative value: ones that come in and scoop advertised loss leaders but don't buy anything else while in the store.  A retailer may devise strategies to minimize traffic from this segment.\n",
    "\n",
    "#### Recommenders\n",
    "Online retailers in particular have developed sophisticated methods for recommending products to customers based on their online history, interests or similarity to other customers.  Online recommendation engines seek to match consumers behavior profiles with those of others and to recommend the products whose similar customers have bought in the past.\n",
    "\n",
    "#### Market Basket Analysis\n",
    "Market Basket Analysis is a technique where retailers looks for correlations in items that consumers tend to buy at the same time.  This kind of analysis can be used for recommendations, but also as a way of maximizing profit.  Some items naturally go together, like burgers and condiments, so a grocery store can advertise a special on ground beef but be sure to have all of the condiments at full price that week.  Retailers continually mine store data for more subtle associations between items and use the insights to inform their pricing strategy.\n",
    "\n",
    "#### Churn Prevention\n",
    "It's an old saying that it takes about ten times as much money to acquire a new customer as it does to retain an existing one.  Retailers and services providers such as Telcos are deeply interested in ways to boost brand loyalty or intervene meaningfully when a customer is considering switching to another brand.  In seeking churn prevention some companies mine the data they have on the behavior of their customers or their comments in social media in an attempt to find markers of customers at risk of defection so they can attempt to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Financial Services\n",
    "#### Fraud Detection\n",
    "Fraud detection is serious business in the online world.  Online marketplaces do real-time fraud detection where a decision whether to allow a transaction to take place between two parties on the network must be made in a fraction of a second.  Modern fraud detection systems consider a wide variety of factors such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the locations of the two parties to the transaction\n",
    "* whether either of the users has a new account but is using the app's features like an experienced user\n",
    "* whether either of the accounts is linked with other accounts in a fashion that would facilitate money laundering\n",
    "* with whom the two parties have other relationships\n",
    "* whether the names and addresses of the parties are credible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Financial Markets\n",
    "Attempting to \"beat the market\" is as old as exchanges and people continue to build increasingly more sophisticated predictive models to attempt to do so.  Some also incorporate other AI techniques to try to discover positive or negative impacts from electronic news releases and execute trades within a fraction of a second, faster than people can react."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications in Healthcare\n",
    "#### Diagnosis\n",
    "\n",
    "Several startups are developing diagnoses for various kinds of cancer by analyzing skin photos or breath samples, using predictive models trained to recognize telltale signs.  *Deep Learning*, a branch of Machine Learning that has revolutionized face and object recognition in photos, is now being used to analyze X-ray photos for signs of disease. \n",
    "\n",
    "\n",
    "#### Drug Discovery\n",
    "Other startups are using Machine Learning to identify potential drug candidates by analyzing patterns in the chemical properties of previously successful and unsuccessful drugs.  Lab testing of compounds is a slow and costly process which can be dramatically accelerated by focusing the search on compounds most likely to have desirable properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In coming weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will be covering a broad introduction to Data Science over the next few weeks.  Through the course (and overall program) we will be working in the *Python* programming language, which has become the number one choice for Data Science, slightly ahead of *R*.  We will begin by learning the basics of Python, then focus on the **Numpy** (Numpy developers, 2018), **Pandas** (Pandas community, 2018), and **Scikit-Learn** (Scikit-learn developers, 2018) libraries for Statistics and Machine Learning.  As you build your skills, you will use them to characterize datasets and begin building simple predictive models using these tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "You have reached the end of this module. \n",
    "\n",
    "If you have any questions, please reach out to your peers using the discussion boards. If you and your peers are unable to come to a suitable conclusion, do not hesitate to reach out to your instructor on the designated discussion board.\n",
    "\n",
    "When you are comfortable with the content, and have practiced to your satisfaction, you may proceed to any related assignments, and to the next module. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Deep Learning, (nd). wikipedia, the free encyclopedia. Accessed Sept 4, 2018.  [online](https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=850422779#History)\n",
    "\n",
    "Downey, A. (2015). Think python--how to think like a computer scientist. [online](http://greenteapress.com/wp/think-python-2e/)\n",
    "\n",
    "NumPy developers, 2018. Numpy. [online](http://www.numpy.org)\n",
    "\n",
    "Pandas community, 2018. Pandas. [online](http://pandas.pydata.org/index.html)\n",
    "\n",
    "Scikit-learn developers, 2018. Scikit-learn: machine learning in python — scikit-learn 0.19.2 documentation. [online](http://scikit-learn.org/stable/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
